# PhD Update 9/10/23
## Update
Continued work on proof search in LeanDojo
- Implemented/tested proof search state representation to allow for multiple approaches to be implemented and tested
    - Separates subgoals, tracks cycles, ancestors, errors, context (other goals needed), visit_count and children
    - Designed to minimise redundancy (i.e. if a proof is possible from what has been done, should be found)
    - Records as much as possible for learning algorithms
- Implemented initial synthetic data collection
    - Recording all found paths for goal and all subgoals, proved/failed, time spent in Lean, visit count (including descendants), distance_to_proof
- Spent time planning/brainstorming how best to learn from the available data
    - 3 different models to learn:
    - Tactics
        - RLHF style learning should be possible for tactics conditioned on the state
        - Could rank all proofs for a given goal (note that many options are found in search, taking just the best would discard a lot of good data)
            - Rank first by steps to proof, then the total time the tactic took to execute in Lean
            - Can create reward from this ranking for use with PPO, possibly scaling by the visit_count/num_proofs of the node as a proxy for difficulty
                - E.g. if 10000 visits, only one found proof then reward = 10000, if 100 visits and 10 proofs found, then reward = 10
            - If a tactic leads to an error, give a constant negative reward
            - Still tbd exactly what approach to use
    - Goal selection data
        - HTPS use 1 if proven, 0 if failed (all tactics error), and use the current policies estimate as the ground truth
        - Could take a similar approach, using visit_count to approximate difficulty
            - E.g. P(Provable | visits) ~ 1 / sqrt(visits) for open goals
    - Retriever
        - Trained on well annotated human proofs, label 1 if used, randomly sample premises from same file and from all candidates to label 0
        - Keep the same for now, for synthetic data label positive as before (if used in a proof), and draw negatives from root goal
        - (todo) need to record full trace of which premises are used in retriever

## TODO
- Incorporate into BAIT (enough to run ReProver on LeanDojo with BAIT config/monitoring setup)
- Continue collecting synthetic data 
- Look more into PPO training, start writing tactic and goal retraining code (ignore retriever for now)