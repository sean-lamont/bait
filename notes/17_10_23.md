# PhD Update 17/10/23
## Update
- Initial results for 'random' test set in LeanDojo using new representation
    - Original ReProver (no retrieval): pass@1 = 0.583, 1148 proved, 821 failed, 31 non-theorems discarded
    - With subgoal separation, no other change: pass@1 = 0.6, 1180 proved, 788, 32 non-theorems discarded
     
- Implemented UpDown for evaluation, with two approaches for training the goal selection model 
    - 'Simple' approach of predicting PROVABLE/UNPROVABLE token given a state
        - Dataset from all proven goals, and any unproven goals over a visit threshold set to have a 0 label
        - Then use this score for UpDown processing 
        - For training, 57% negatives, 43% positive, negatives taken as unproven goals if over 256 visits. 
          - Best accuracy approx. 89% 
        - So far, performs worse than BestFS (~50% proven vs 60%)
          - Slower (~20%) as scores are computed for goals as they are generated,
          so timeout occurs quicker
          - From looking at the proof attempts, seems to be biased towards breadth
            - Hypothesis: This is due to there being no distinction between easy and hard proofs.
              This leads to hard, low depth goals being treated the same as deeper goals which are easier
         
    - Following the Polu et al. approach in curriculum learning,
  a better approach could be to predict the number of steps remaining
        - Done in practice by bucketing the proof lengths, including an infinite size for unproven goals
        - Proof 'score' the computed as the normalised, weighted (by model logit/probs) score over all buckets, giving a score in [0,1]
        - On small dataset so far, can get ~83% accuracy for tokens. Most ground truth scores are either 0 or 1, so instead 
      validating with MSE using normalised score vs ground truth
        - Time for proof could be a better metric, however hard to standardise across hardware
          - Can be optimised in tactic model by ranking/filtering best result based on time
           
- Thought about how to do ranking/queue search model
  - Usually expand 1 goal fully at a time, when the first expansions may be all it needs
  - To address this, can add a ranking model to filter the best global (g,t) pairs
    - From a queue, the environment runs the top (g,t) pairs
    - The output is new goals, which are scored by a goal scoring model
    - The updated proof state, with scores, is used to select the best goal(s), using e.g. UpDown
    - The best selected goal(s) are then sent to have tactics generated, where the tac_gen model finds k tactics for the goal
    - The k (g,t) pairs are then ranked with a ranking model. 
      - Simplest is to use the logprob of the model
      - Alternatively, can train a ranking model on edges (g,t) to predict the number of steps as with the scoring model
      - Once a goal has tactics generated, the score S(g) is updated to be the best tactic over (g,t). 
        - Some of these will be expanded, some won't. If expanded, the score is based on the destination nodes, otherwise based on the ranking model R(g,t) 
    - The global queue is updated with the scores, which the environment uses to repeat the process
      - Can separate threads for the environment and models for better performance
    - Slightly more formally:
      - Up Step (Difficulty of proving g from any observed path)
          - S(g) 
            - = GoalModel(g) if g is new
            - = min_t S(g, t) for out_edges t if (g,t) has been computed by TacGen(g)
          - S(g, t)
            - = RankingModel(g,t) if (g,t) unexplored
            - = Product(S(g')) for g' in Siblings(g,t) where Siblings(g,t) are the subgoals from Env(g,t)
      - Down Step (Difficulty of proving goal given (g,t) multiplied by the difficulty of (g,t))
          - FinalScore(g, t) = RankingModel(g,t) * Context(g)
            - = RankingModel(g,t) * Product(S(g')) for g' in Context(g)
          - FinalScore(g) = GoalModel(g) * Context(g)

- Overall goal 
  - Compare current proof search methodologies (fits in well with embedding comparisons)
    - HTPS, Proof Step classification are SoTA from OpenAI/Meta, BFS from Google, BestFS from LeanDojo
  - Hopefully find a better approach
  - Insights into best approach, limitations (e.g. alpha-equivalent expressions exploding search space)
        
## TODO
- Train and test 
- Incorporate into BAIT (enough to run ReProver on LeanDojo with BAIT config/monitoring setup)
- Continue collecting synthetic data 