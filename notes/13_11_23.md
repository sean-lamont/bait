# PhD Update 24/10/23
## Update
- Continued writing end-to-end code 
  - Training classes for tactic and goal generation 
  - Setting up training to run automatically after an evaluation loop, retraining with newly observed data
  - Processing logs after an evaluation loop
    - Adds full proof trace (goal, tactic, response, goal_prob, tac_prob)
    - Also preprocessing for goal selection (proof length or binary) and for DPO (preferences)
  - Set up distributed training pipeline with MongoDB + Ray dataloading 
    - Very fast, still profiling however it's much faster than loading from disk (pickle/torch), 
     and doesn't need whole dataset loaded into memory
    - Working through some issues with synchronisation of datasets across GPUs
  - Wandb and hydra integrated, so logging/config management is more consistent
- Added BestFS to abstract pipeline 
- Started writing HTPS
- Proof visualisation 
  - Useful for debugging
  - Can see full trace and order of new nodes
  - More options (e.g. expanding/collapsing nodes) can be added


## Notes
- If harder goal proven, would be nice to automatically update easier (i.e. goal which has fewer hypotheses, somthing implied by proven goal)
- Proof search diffusion? Insert/delete, start from 'random proof' as noise, refine to better proof.
  - Error term prediction between subsequent steps, start from noise, follow this error prediction to refine 
  - Start with root, noise corruption is leading edge, denoising is adding edge in 
- LLM probably lower hanging fruit
## TODO
- Complete End-to-end training code
  - Finish DPO implementation
  - Run with BestFS and with UpDown initially 

- Finish HTPS, BFS 