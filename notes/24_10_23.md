# PhD Update 24/10/23
## Update
- Trained ranking model, noted that it doesn't learn much beyond a random model
  - Seems to be a hard task
  - Can automatically be done by finetuning the tactic generation model with e.g. RLHF anyway
  - Found that the goal model takes a small amount of time anyway, even when removing the restriction of 
    a fixed expansion size
  - Best and simplest approach then seems to be just have a goal selection and a tactic generation model
   
- Implemented setup removing a fixed tactic expansion budget per goal
  - Agent selects best goal, then applies a single tactic
  - When a new goal is selected, 64 tactics are generated and queued for if it is selected again
  - Currently running, results tbd. From looking at output, can get to the correct proof much quicker
  - However, noted a problem with 'DFS' style traversal where the model overestimates the ease of a goal and its children
        
## TODO
- Continue running experiments for updown/sequential goal selection
- Start implementing HTPS