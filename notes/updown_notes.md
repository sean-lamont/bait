Main problem with UpDown is overconfidence in model, leading to underexplored paths
  - Possibly want to bias exploration at a higher level? (done implicitly with HTPS)
- How to add exploration in UpDown, and avoid overconfidence in model??
  - Can we use visit count in some way? e.g. take visit count that would have otherwise come from htps ..
    - Don't think so, how do you account for e.g. great uncles etc. 
   
[//]: # (  - Should be related to visit count and descendants, as new information is found should be able to update predicted score)
[//]: # (  - Take mean instead of max, then high scores will eventually come down quicker and won't be skewed by high predictions)
[//]: # (    - Ignoring edges leading to errors, otherwise will bias nodes which are easily expanded)
[//]: # (    - Either mean over direct children, or mean over all descendants &#40;HTPS does all descendants&#41;)
[//]: # (    - Doing this stops easily finding the best fringe though... &#40;since up_score will not be the same for parent and best descendant&#41;)


  - Initial score weighted by prior, e.g. 0.5 or tac_gen logprob 
    - Sample from this
  - Add exploration epsilon (weighted by e.g. depth or visit count)

- Score conditioned on previous goal?
  - Removes need for depth penalty, only score 1 if provide progress
  - No need for a notion of easier/harder
  - Synthetic data with variable renaming, adding redundant hypotheses etc. (get model to generate)
  - Get all edges with subgoals, take those which lead to proofs, and any other edges with visit count >= proven edge
  is a 0 label?


- Idea: Use MLM (e.g. Falcon, or just GPT-4) to generate synthetic goals which are informative about goal difficulty
  - Should be invariant to e.g. variable renaming, rearranging, redundant hypotheses
  - Feed it examples of proven and unproven goals (over a certain visit count), get the model
    to use the characteristics of the expression to generate goals which are hard or low score, and good or high score
    - Low score could also include e.g. multiple stacked redundant hypotheses, goals which don't make progress

- "You are an expert in Theorem Proving using Lean. You are given an initial proof state, and a sub-goal which was generated by applying a tactic to the initial state.
  Your task is to determine whether the sub-goal provides meaningful progress to the proof of the original goal.
  Examples of no progress include variable renaming, redundant hypotheses, rearranging expressions, and obviously false goals (e.g. 1 = 0).
  Meaningful progress is when the sub-goal is strictly easier than the initial goal, such as when hypotheses are removed or the goal itself is easier.
  Provide your output as a score from 0-1, where 0 is high confidence of no progress, 1 is high confidence of meaningful progress, and 0.5 is uncertain.
  Along with the score, provide a brief explanation as to your reasoning in the format (score, reason). For example, (0.1, 'redundant hypothesis'), or (0.99, 'trivially true')"
  The goals will be given to you in the format (Initial: [initial_proof_state], Sub-Goal: [Sub-goal]). Is this clear, or do you have any questions?"


- Idea 2: Goal scoring conditioned on previous or root goal
  - Can now score goal based on whether it progresses the proof, or is useful to the proof,
    rather than just 'provable'
    - Unlike provable/not provable, this doesn't suffer from a notion of 'hard' or 'easy' (i.e. provable doesn't distinguish very difficult vs very easy)
      - Instead, gives a score based on how well it progresses the original goal
      - Makes it easier for synthetic data (variable renaming, rearranging, redundant hypotheses) with 0 labels
      - Any node leading to a proof gives a 1 label, node over a visit count still 0(?), or use MLM for synthetic data
        with 0 labels for no progress
      - Then model will select goals which seem to give progress

- Eventually want tac gen conditioned on previously generated tactics. E.g. give goal, and list of previous tactics,
  - Somehow reward diversity, e.g. label 0 if we get the same hypotheses, or
  - Could add error message of failed tactics as part of state, and tactics which have failed downstream
