# PhD Update 19/9/23

## Proof Search
### Full Fringes
Algorithm for computing all current fringes, based on graph based goal representation.

Every time a new goal is added or proved, the graph is updated and fringes recomputed.

Advantages over standard fringes:
-  Contains all possible sets of fringes based on the currently observed goals. 
The original approach generates one new fringe per step, replacing the chosen goal to expand.
With the graph representation, we can instead find all possible sets of paths.
- Propagates proven goals. Previous fringes are immutable over time, whereas once a goal is proven
the graph representation will propagate this upwards, removing any redundant siblings and child nodes to
give the fully reduced set of possible candidates.

Disadvantages:
- Requires computation of fringes which can be expensive. 
- A larger amount of tracking is required for replays, since the graph changes over time,
so the candidate fringes must be saved every step. 

### UpDown
Rather than selecting the best fringe, this approach selects the best goal directly.
It does this by taking the product of the goal itself, and what else needs to be proven in addition to the goal. 
The parent nodes of other goals that need to be proven are tracked, and a score for each of these is maintained.
The simplest choice is the maximum over the node itself, and all children from the node (where each child is the product
of scores of every sibling). 

### Progress
Full fringe approach implemented. A lot of time debugging this, as many fundamental changes to TacticZero had to be made
including:
- Replays and proof history 
- Tracking previously taken actions 
- Dealing with duplicate goals in the graph (currently, do not add a duplicate)

From initial tests, performance seems to reduce, so I've spent a lot of time testing 
every part of the implementation. Things like comparing the new and old fringes generated step by step, 
checking the replays are correct, verifying the computational graph generated for backprop etc. Still no 
obvious problems. 

When the steps are restricted to 5, the performance seems to be roughly the same, compared to 50.
Since there are many more nodes and fringes to track, not surprising if there's a problem with the goal selection...

## LeanDojo

### ReProver
Uses Dense Passage Retriever (DPF), trains retriever on ground truth, to minimise the distance between an expression and premises used in its 
proof. Closest expressions in embedding space are then selected, and added as context for the language model to use.

Tactic Selection is done with a fine-tuned byte-level Transformer, without a fixed tokenisation.
Saves a lot of effort in preprocessing pipelines, and suitable for the complex unicode tokens in Lean. 
Takes the current tactic state, and retrieved premises as the context and predicts the next full tactic.

Proof search: BestFS, based on sum of log-likelihoods of tactics leading to a state.
- Discourages deep proofs with the summation of log-likelihoods
- Subgoals aren't separated into separate nodes, each tactic expands a state with all siblings


Notes: 
- Retrieval efficient, with 30K average premises allowed per proof. Compared to TacticZero setup with about 2K.
- Doesn't consider tactics when ranking the premises. 
- Small LLM, publicly available. 
- Without fine-tuning, performance is poor. 
- Human only proof data, no synthetic/end-to-end setup

Extensions:
- Differentiable Search Index (DSI), also mentioned in the paper, as an extension to 
simple dual encoder schemes such as DPF. Along with other benefits, should also allow us
to include the tactic in the query.
- Larger model, e.g. Falcon or recent small "Textbooks are all you need" Phi-1.5 model
- Better Proof Search (i.e. HTPS, Fringe, UpDown)
  - Need end-to-end samples for training, to label negative goals
- End-to-end/RL

### Progress
- Installed and ran LeanDojo and the Transformer based ReProver. 

## TODO
- Proof search HOL4 debugging?
- Replicate LeanDojo results 
- Start implementing proof search algos on LeanDojo
