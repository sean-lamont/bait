# PhD Update 16/1/23

## Update 
- Retriever model running/integrated
  - Not using in current model for now, as it's tbd how to train on synthetic data
- Added BFS and Fringes to search
  - Still need to test and check it works properly 
   
- seq2seq experiment 
  - Trains on live environment data, optimising seq2seq CE loss with only proven (sub)goals 
  - Evaluating on end-to-end environment, looks like there is some performance gain 
  - Currently running 2nd iteration, using best model after the 1st iteration of synthetic training



## TODO
- Continue running seq2seq
- Look at integrating HOL4 into End to End training 
- Go back to ILQL experiments
  - Need to implement beam search/sampling for this

- Add holdout/test logprob of ground truth
  - Other metrics to monitor?
    - Length of generated sequence
    - BLEU scores/sample diversity (good since we have GT)
    - ROUG
    - SacreBLEU library
    - L2 norm of difference between parameters of original and finetuned model