# PhD Update 27/11/23

### Previous TODO 
- [ ] Test and compare HTPS and UpDown
  -  [x] Visualise proofs and the scores given from each approach 
- [ ] Try get DPO stabilised for evaluation, and test performance
- [x] Check scores from top-p

#### Sampling
- Top p scores from nucleus sampling are normalised to new distribution, making them invalid 
- Current approach to properly sample without any modifications
  - Beam search will always generate 64 unique sequences
  - Sampling does not guarantee this, and in many cases <10 are generated
  - Initially kept looping until 64 were reached, but cases where the model is very confident leads to hanging
  - Solution for now: loop 10 times, returning however many tactics are generated, tradeoff between time and # sequences
- Overall sampling is slower and seems to perform worse, however leads to data faithful to the DPO assumptions

#### HTPS/ UpDown
- Updated the tracing of UpDown and HTPS, allowing for better visualisations and debugging
  - Better visualisation, can now interactively view the proof and search data over time 

- Good example of updown not exploring, only one edge from the root node is explored,
   as the others are initially estimated very low:
    - `"../experiments/runs/leandojo/sample_bestfs_2023_11_29/20_30_17/traces/tsub_lt_tsub_iff_left_of_le")`
  - As visit count is updated in HTPS, more likely to explore nodes earlier in tree 
  - UpDown is greedy, so more prone to getting trapped by bad estimates of provability
    - Could update score dynamically as explored. E.g. as num descendants grows, more confident in difficulty, so lower score for all children 
    - Depth penalty insufficient (e.g. depth 5 still has 64^5 nodes, with minimal penalty, only punishes very deep paths, doesn't consider cousins)
     
  
- Again, found a very large number of cases where variable renaming explodes the graph
  - Could just run a search over all children from a given node, renaming hypotheses and their instances, 
  then taking the set over these 
    - Need to be able to properly tokenise (i.e. get hypothesis identifiers (already done), tokenise and search for these in the expression)
     


#### DPO
- Updated training to evaluate end-to-end rather than with proxy metric
- Generated data from end-to-end prover with sampling instead of beam search
- Updated generation of pairs 
  - Still TBD what the best way is, for now, sorting by closest length sequence 
  - Restrict number of pairs for a given goal
- Currently running, seems to worsen performance (blows up tactic generation time)
 
## TODO 
- [ ] 