# PhD Update 16/1/23

## Update 
- Website + Docs finished
  - Leaving full End-to-End writeup for when it's more complete

- seq2seq experiment running
  - Trains on live environment data, optimising seq2seq CE loss with only proven (sub)goals 

- Read AlphaGeometry paper

- Started integrating retriever
  - Current environments follow roughly a similar pattern, so an abstract retriever looks possible
  - Just need to pass around some (environment specific) context object, and implement a 
  `get_premises` function for each environment


## TODO
- Keep running seq2seq 
  - Try to see if we can get some improvement in overall proving performance 
- Integrate HOL4 into End to End training 
- Add retriever based model
- Continue with proof search experiments

- Testing hypothesis of LLM improving based on error messages ( how to construct some small test dataset?)
- Adding in a nodes ancestors as context (chain of thought style prompting)