# PhD Update 21/11/23

## Update

## Summary of current progress

### BAIT
Aim/motivation is as outlined in the first paper; it is difficult to compare
learning methods in the area given private code, and the variety of approaches 
split across different proving systems.

The first paper setup this problem, and noted the axes on which these approaches vary:
the learning approach, proof search method, tactic generation, embedding architecture.
The previous iteration inlcuded embedding models, and TacticZero.

Since then, a large focus has been extending this system to include more of these components.
LeanDojo has been helpful since it gives us a new environment, as well as generative tactic models
and retriever based approaches.

A summary of the additions are:

- Generative Models
  - Working with ByT5 from LeanDojo as a base here
- Proof search tree representation
  - Subgoal separation, better tracking of proven goals, observed small improvement with BestFS
- End to end loop
  - Faster and more resource efficient multiprocessing
  - More abstract setup that LeanDojo or previous BAIT version, 
  should make it easier to add HOList and HOL4, possibly others
- Better visualisation
- Processing proof logs 
  - Enables Goal selection, DPO 
- Environments

### Proof search

Goal Scoring Model used in several approaches,
learnable model mapping a goal to a score for use in search.
Still open question what the best approach is

- Binary Provable/Unprovable (HTPS)
- Proof Length (Curriculum Learning)

No Goal Model:

- BestFS
  - LeanDojo Default, uses cumulative logprob from tactic model
- BFS
  - Breath first, needs no outside information 

Goal Model:

- HTPS
  - Uses the mean score over children 
  - Fixed expansion budget per node 
  - Finds best fringe under the mean, with some exploration 
- UpDown
  - Uses 'max' over children to score a node, and product over siblings 
  - Can backtrack
  - Can sample from fringes directly by their score 
 
Progress:
- UpDown implemented and tested after 1 evaluation loop, no benefit to performance so far 
- HTPS implemented yet to be tested
- Plan is to test HTPS vs UpDown with the same goal model (binary, as used in their paper)

### DPO
Fine-tuning tactic model based on responses from environment.
Given the natural ordering, can use DPO to fine-tune tactic generation 

Currently training with LoRA, can get >90% win rate however the first iteration led
to degenerate tactics (similar to the problem mentioned in the paper)

(christian) 
- normal sampling rather than beam search
- Not needing as many data points
- Better way of filtering (could be too biased towards unproven goals with lots of nodes?)
- Inference evaluation (running model on proofs in our case)
- If you start with good pretrained model, surprising how few steps you need for fine tuning
- Take normal samples, construct pairs which are approximately equal length but not similar 
  - Heuristic not too similar? (Edit distance, bleu)
  - Length could be a problem with the DPO objective


### TODO (~2-3 months)
- Test and compare HTPS and UpDown
  - Visualise proofs and the scores given from each approach
- Try get DPO stabilised for evaluation, and test performance
- Add retrieval based model
- Add HOList, HOL4 