@inproceedings{vaswani_attention_2017,
    author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
    booktitle = {NeurIPS},
    editor = {},
    pages = {},
    publisher = {},
    title = {Attention is All you Need},
    url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
    year = {2017}
}


@inproceedings{wu_tacticzero_2021,
    title = {{TacticZero}: {Learning} to {Prove} {Theorems} from {Scratch} with {Deep} {Reinforcement} {Learning}},
    shorttitle = {{TacticZero}},
    url = {https://proceedings.neurips.cc/paper/2021/hash/4dea382d82666332fb564f2e711cbc71-Abstract.html},
    abstract = {We propose a novel approach to interactive theorem-proving (ITP) using deep reinforcement learning. The proposed framework is able to learn proof search strategies as well as tactic and arguments prediction in an end-to-end manner. We formulate the process of ITP as a Markov decision process (MDP) in which each state represents a set of potential derivation paths. This structure allows us to introduce a novel backtracking mechanism which enables the agent to efficiently discard (predicted) dead-end derivations and restart the derivation from promising alternatives. We implement the framework in the HOL theorem prover. Experimental results show that the framework using learned search strategies outperforms existing automated theorem provers (i.e., hammers) available in HOL when evaluated on unseen problems. We further elaborate the role of key components of the framework using ablation studies.},
    urldate = {2023-07-25},
    booktitle = {NeurIPS},
    author = {Wu, Minchao and Norrish, Michael and Walder, Christian and Dezfouli, Amir},
    year = {2021},
}


@inproceedings{wang_premise_2017,
    title = {Premise {Selection} for {Theorem} {Proving} by {Deep} {Graph} {Embedding}},
    url = {https://papers.nips.cc/paper_files/paper/2017/hash/18d10dc6e666eab6de9215ae5b3d54df-Abstract.html},
    abstract = {We propose a deep learning-based approach to the problem of premise selection: selecting mathematical statements relevant for proving a given conjecture. We represent a higher-order logic formula as a graph that is invariant to variable renaming but still fully preserves syntactic and semantic information. We then embed the graph into a vector via a novel embedding method that preserves the information of edge ordering. Our approach achieves state-of-the-art results on the HolStep dataset, improving the classification accuracy from 83\% to 90.3\%.},
    urldate = {2023-06-26},
    booktitle = {NeurIPS},
    publisher = {},
    author = {Wang, Mingzhe and Tang, Yihe and Wang, Jian and Deng, Jia},
    year = {2017},
}


@inproceedings{
luo_transformers_2023,
    title = {Transformers over Directed Acyclic Graphs},
    author = {Yuankai Luo and Veronika Thost and Lei Shi},
    booktitle = {NeurIPS},
    year = {2023},
    url = {https://openreview.net/forum?id=g49s1N5nmO}
}


@inproceedings{chen_structure-aware_2022,
    title = {Structure-Aware Transformer for Graph Representation Learning},
    author = {Dexiong Chen and Leslie Oâ€™Bray and Karsten M. Borgwardt},
    booktitle = {ICLR},
    year = {2022},
    url = {https://api.semanticscholar.org/CorpusID:246634635}
}



