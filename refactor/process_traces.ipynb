{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 592,
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "import glob\n",
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def get_traces(path):\n",
    "    files = glob.glob(path)\n",
    "\n",
    "    traces = []\n",
    "    for file in files:\n",
    "        with open(file, \"rb\") as f:\n",
    "            trace = pickle.load(f)\n",
    "            traces.append(trace)\n",
    "    return traces\n",
    "\n",
    "\n",
    "traces = get_traces('../traces_2023-10-31_17:27/*')\n",
    "traces.extend(get_traces('../traces_2023-10-31_17:28/*'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T08:54:17.011939860Z",
     "start_time": "2023-11-01T08:51:48.524991520Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "outputs": [
    {
     "data": {
      "text/plain": "0.5886524822695035"
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from refactor.proof_node import Status\n",
    "\n",
    "len([t for t in traces if t.tree.status == Status.FAILED])/ len(traces)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fcc721a2b3711b8e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ec81f47539210b60"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6862daf0e2e89b1d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.cuda.mem_get_info('cuda:0')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4e6715a1794d4c7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from refactor.proof_node import InternalNode, ErrorNode, ProofFinishedNode"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client['lean_dojo']\n",
    "collection = db['goal_data']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "def add_goal_data(node, visits):\n",
    "    steps = node.distance_to_proof\n",
    "    # todo add up_score as new estimate? only after certain visit_threshold\n",
    "\n",
    "    datum = {\n",
    "        'goal': node.goal,\n",
    "        'distance_to_proof': steps,\n",
    "        'visits': visits[node.goal],\n",
    "        'local_visits': len(node.out_edges) if node.out_edges else 0,\n",
    "        'score': node.up_score.item() if isinstance(node.up_score, torch.Tensor) else node.up_score\n",
    "    }\n",
    "\n",
    "    return datum\n",
    "\n",
    "for trace in traces:\n",
    "    nodes = trace.nodes\n",
    "\n",
    "    updated_visit_count = {node: nodes[node].visit_count for node in nodes}\n",
    "\n",
    "    for goal, node in nodes.items():\n",
    "        for a in node.ancestors:\n",
    "            updated_visit_count[a] += node.visit_count\n",
    "\n",
    "    for node in nodes:\n",
    "        step_datum = add_goal_data(nodes[node], updated_visit_count)\n",
    "        if step_datum:\n",
    "            collection.insert_one(step_datum)\n",
    "            # goal_step_data.append(step_datum)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collection = db['edge_data']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_edge_data(trace):\n",
    "    data = []\n",
    "    for i, ((goal, goal_prob), edge) in enumerate(zip(trace.search_trace[:len(trace.tac_trace)], trace.tac_trace)):\n",
    "        datum = {\n",
    "            'iteration': 0,\n",
    "            'step': i,\n",
    "            'top_goal': trace.theorem,\n",
    "            # 'goal': goal,\n",
    "            'goal': goal.goal,\n",
    "            'tactic': edge.tactic,\n",
    "            # 'goal_prob': goal_prob,\n",
    "            'goal_prob': goal_prob.item(),\n",
    "            'tac_prob': edge.logprob,\n",
    "            'distance_to_proof': edge.distance_to_proof(),\n",
    "            'visits': edge.visit_count(),\n",
    "            'time': edge.time,\n",
    "        }\n",
    "        # add children of edge\n",
    "        if len(edge.dst) == 1 and isinstance(edge.dst[0], ErrorNode):\n",
    "            # todo could record error message for e.g. self-correcting proof approach>\n",
    "            datum['outcome'] = ['Error']\n",
    "        elif len(edge.dst) == 1 and isinstance(edge.dst[0], ProofFinishedNode):\n",
    "            datum['outcome'] = ['Proven']\n",
    "        else:\n",
    "            outcome = [d.goal for d in edge.dst]\n",
    "            datum['outcome'] = outcome\n",
    "        data.append(datum)\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# edge_data = []\n",
    "\n",
    "for trace in traces:\n",
    "    collection.insert_many(get_edge_data(trace))\n",
    "    # edge_data.extend(get_edge_data(trace))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def add_rand_idx(collection):\n",
    "    collection.update_many({'rand_idx': {'$exists': False} },\n",
    "        [{'$set':\n",
    "                {'rand_idx': {\n",
    "                    '$function': {\n",
    "                        'body': 'function() {return Math.random();}',\n",
    "                        'args': [],\n",
    "                        'lang': \"js\"\n",
    "                    }\n",
    "                    }}\n",
    "        }]\n",
    "        )\n",
    "\n",
    "    collection.create_index('rand_idx')\n",
    "    return\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rank_collection = db['tac_ranks']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def transform_goal(goal_datum, max_len=10, visit_threshold=2048):\n",
    "    proof_len = goal_datum['distance_to_proof']\n",
    "    if proof_len < max_len:\n",
    "        return {'goal': goal_datum['goal'], 'target': (max_len + 1) - goal_datum['distance_to_proof']}\n",
    "    elif proof_len < math.inf:\n",
    "        return {'goal': goal_datum['goal'], 'target': 1}\n",
    "    elif goal_datum['visits'] >= visit_threshold:\n",
    "        return {'goal': goal_datum['goal'], 'target': 0}\n",
    "\n",
    "\n",
    "# create pairs of winners/losers based on edges from a given goal, and maintain tac probs for each\n",
    "# e.g. edges = find({'goal': 'goal'}).edges\n",
    "def rank_edges(goal, edges):\n",
    "    valid_edges = [edge for edge in edges if edge['outcome'] != ['Error']]\n",
    "    invalid_edges = [edge for edge in edges if edge['outcome'] == ['Error']]\n",
    "\n",
    "    # rank all valid_edges above all invalid_edges\n",
    "    # w_l = [(goal, w['tactic'], w['tac_prob'], l['tactic'], l['tac_prob'], 'valid') for w in valid_edges for l in invalid_edges]\n",
    "    w_l = [{'goal': goal, 'winner': w['tactic'], 'w_prob': w['tac_prob'], 'loser': l['tactic'], 'loser_prob': l['tac_prob'], 'type': 'valid_rank'} for w in valid_edges for l in invalid_edges]\n",
    "\n",
    "    # print (f'{len(valid_edges), len(invalid_edges)} valid/invalid')\n",
    "    # print (len(w_l))\n",
    "    # from valid_edges, rank proven goals above non_proven valid goals\n",
    "    proven_edges = [edge for edge in valid_edges if edge['distance_to_proof'] < math.inf]\n",
    "    success_non_proven_edges = [edge for edge in valid_edges if edge['distance_to_proof'] == math.inf]\n",
    "\n",
    "    # w_l.extend([(goal, w['tactic'], w['tac_prob'], l['tactic'], l['tac_prob'], 'proven') for w in proven_edges for l in success_non_proven_edges])\n",
    "\n",
    "    w_l.extend([{'goal': goal, 'winner': w['tactic'], 'w_prob': w['tac_prob'], 'loser': l['tactic'], 'loser_prob': l['tac_prob'], 'type': 'proven_rank'} for w in proven_edges for l in success_non_proven_edges])\n",
    "    # from proven edges, rank based on distance_to_proof, then execution time\n",
    "    ranked_proofs = sorted(proven_edges, key=lambda x: (x['distance_to_proof'], x['time']))\n",
    "\n",
    "    w_l.extend(\n",
    "        [{ 'goal': goal, 'winner': ranked_proofs[i]['tactic'],\n",
    "           'winner_prob': ranked_proofs[i]['tac_prob'],  'loser': ranked_proofs[j]['tactic'], 'loser_prob': ranked_proofs[j]['tac_prob'],\n",
    "           'type': 'time_len_rank' } for i in range(len(ranked_proofs)) for j in\n",
    "\n",
    "         range(i + 1, len(ranked_proofs))])\n",
    "\n",
    "    # among successful without proof, rank those that lead to the same outcome based on time only\n",
    "    for i, edge in enumerate(success_non_proven_edges):\n",
    "        same_outcome_ranks = []\n",
    "        for j in range((i + 1), len(success_non_proven_edges)):\n",
    "            edge_2 = success_non_proven_edges[j]\n",
    "            if set(edge['outcome']) == set(edge_2['outcome']):\n",
    "                if edge['time'] < edge_2['time']:\n",
    "                    # same_outcome_ranks.append((goal, edge['tactic'], edge['tac_prob'], edge_2['tactic'], edge_2['tac_prob'], 'same_outcome'))\n",
    "                    same_outcome_ranks.append({'goal': goal, 'winner': edge['tactic'], 'winner_prob':  edge['tac_prob'], 'loser': edge_2['tactic'], 'loser_prob':  edge_2['tac_prob'], 'type': 'same_outcome'})\n",
    "                else:\n",
    "                    same_outcome_ranks.append({'goal': goal, 'winner': edge_2['tactic'], 'winner_prob':  edge_2['tac_prob'], 'loser': edge['tactic'], 'loser_prob':  edge['tac_prob'], 'type': 'same_outcome'})\n",
    "                    # same_outcome_ranks.append((goal, edge_2['tactic'], edge_2['tac_prob'], edge['tactic'], edge['tac_prob'], 'same_outcome'))\n",
    "\n",
    "        w_l.extend(same_outcome_ranks)\n",
    "\n",
    "    if w_l:\n",
    "        rank_collection.insert_many(w_l)\n",
    "\n",
    "    # winners = [(a[1], a[2], a[-1]) for a in w_l]\n",
    "    # losers = [(a[3], a[4], a[-1]) for a in w_l]\n",
    "\n",
    "    return goal, winners, losers\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_goals = set([edge['goal'] for edge in collection.find()])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# data = []\n",
    "for goal in tqdm(all_goals):\n",
    "    test_edges = [edge for edge in collection.find({'goal': goal})]\n",
    "    goal, winners, losers = rank_edges(goal=goal, edges=test_edges)\n",
    "    # data.append((goal, winners, losers))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "goal, winners, losers = data[8]\n",
    "\n",
    "\n",
    "len(winners)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i = 419\n",
    "print (winners[i])\n",
    "print (losers[i])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i = -6\n",
    "print (winners[i])\n",
    "losers[i]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# todo method to reconstruct search tree based on edge data above\n",
    "# run normal search process, replace run_tac with outcome -> edge, replace get_goals with goal, replace get_tactics with tactic\n",
    "# useful for reward based goal models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# todo how to merge different attempts of same proof?\n",
    "# For goal data, if proof length is lower, take that data point. If failed, and visit count higher, replace with that as well\n",
    "# I.e. every new attempt, add all new goals, and also update existing goals with above criteria\n",
    "\n",
    "# For edge data...\n",
    "# Assume all valid/invalid edges are still valid/invalid, then those rankings are fine\n",
    "# Rankings from proven/success could be changed if success turns out to be a proof..\n",
    "# Rankings within proof could also change, if shorter proof from children is found\n",
    "# Seems small/unlikely for this to make much of a difference. Worst case is a longer proof is ranked better than a shorter/slower one\n",
    "\n",
    "# Don't just keep best trace, since we may discard useful old goals\n",
    "# Best trace given by the trace with the shortest proof...\n",
    "\n",
    "\n",
    "# todo check logits of forward match those from generation\n",
    "\n",
    "# todo train scripts for eval models\n",
    "\n",
    "# todo htps\n",
    "\n",
    "# todo add BFS, bestfs\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
