epochs: 800
max_steps: 5
pretrain: False
pretrain_ckpt: ''#'experiments/runs/premise_selection/2023_07_28/formula_net_19_24_20/checkpoints/epoch=9-acc=0.9163475036621094.ckpt'
proof_db: ['hol4', 'proof_logs']
# tactic_config
exp_config:
  device: [1]
  name: "directed_sat_pretrain"
  resume: False
  experiment: "tactic_zero"
  directory: 'experiments/runs/'#
  logging_config:
    project: "rl_new"
    offline: True
#    id: 'wp30q42c'
    notes: "Directed SAT Pretrained"
optimiser_config:
  learning_rate: 5e-5
model_config:
  vocab_size: 2200
  model_type: 'sat'
  model_attributes:
    vocab_size: 2200
    embedding_dim: 256
    batch_norm: False
    global_pool: 'mean'
    num_edge_features: 200
    dim_feedforward: 256
    num_heads: 4
    num_layers: 2
    in_embed: True
    se: 'formula-net'
    abs_pe: False
    use_edge_attr: True
    dropout: 0.
    gnn_layers: 2
    small_inner: False
    abs_pe_dim: 256
data_config:
  type: "graph"
  source: "mongodb" #directory
  shuffle: True
  data_options:
    filter: ['tokens', 'edge_attr', 'edge_index','attention_edge_index']
    db: 'hol4_original_ast'
    expression_col: 'expression_graphs'
    vocab_col: 'vocab'
    split_col: 'paper_goals'
    environment: 'HOL4'
#  attributes:
  # attention_edge_index: 'directed'
  # pe: 'depth'
#  dir: "data/hol4/graph_attention_data_new"
#val_frequency: 128
