epochs: 800
max_steps: 5
pretrain: False
pretrain_ckpt: ''#'experiments/runs/premise_selection/2023_07_28/transformer_encoder_23_28_47/checkpoints/epoch=6-acc=0.918088436126709.ckpt'
proof_db: ['hol4', 'proof_logs']
# tactic_config
exp_config:
  name: "transformer_pretrain"
  resume: True
  experiment: "tactic_zero"
  directory: "experiments/runs/tactic_zero/transformer_pretrain_2023-07-30-14:34:16"#"experiments/runs/formula_net_5_step_no_pretrain_2023-07-24-14:51:09"
  logging_config:
    project: "rl_new"
    offline: False
    id: 'pcgf8tzy'
    notes: "Test"
optimiser_config:
  learning_rate: 5e-5
model_config:
  model_type: 'transformer'
  model_attributes:
    vocab_size: 2200
    embedding_dim: 256
    dim_feedforward: 256
    num_heads: 4
    in_embed: True
    num_layers: 2
    dropout: 0.
    small_inner: False
    max_len: 1024
  vocab_size: 2020
  embedding_dim: 256
data_config:
  type: "sequence"
  source: "mongodb" # directory
  shuffle: True
  data_options:
    split_in_memory: True
    dict_in_memory: True
    filter: ['full_tokens']#, 'depth']
    db: 'hol4_original_ast'#'mizar40'
    expression_col: 'expression_graphs'
    vocab_col: 'vocab'
    split_col: 'paper_goals'
    environment: 'HOL4'
  attributes:
    max_len: 1024
#  attributes:
  # attention_edge_index: 'directed'
  # pe: 'depth'
#  dir: "data/hol4/graph_attention_data_new"
#val_frequency: 128
