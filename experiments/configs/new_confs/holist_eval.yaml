defaults:
  - optimiser_config
  - logging_config

data_config:
  type: 'graph' # sequence, all
  data_options:
    db: 'holist'
    filter: ['tokens', 'edge_attr', 'edge_index']#, 'attention_edge_index', 'depth']
    vocab_col: vocab
  attributes: {}
#    attention_edge: 'directed'

exp_config:
  name: bow_1_node
  experiment: holist_eval
  directory: 'experiments/runs/${.experiment}/${now:%Y_%m_%d}/${.name}_${now:%H_%M_%S}'#'experiments/runs/holist_eval/2023_08_14/gnn_fast_21_04_48'
  checkpoint_dir: ${.directory}/checkpoints
  accelerator: gpu
  resume: False
  device: [0]

model_config:
  model_type: holist_gnn
  model_attributes:
    gnn_layers: 0
    vocab_size: 1500
    dropout: 0.2
    embedding_dim: 128

logging_config:
  project: holist_eval
  offline: False
#  id: '4cpiaxjb' #None #override for resume
hydra:
  run:
    dir: ${exp_config.directory}
  job:
    chdir: False


# Setting ProverOptions Proto
path_tactics: 'data/holist/hollight_tactics.textpb'
path_tactics_replace: 'data/holist/hollight_tactics_replacements.textpb'
path_theorem_database: 'data/holist/theorem_database_v1.1.textpb'

model_architecture: PARAMETERS_CONDITIONED_ON_TAC
theorem_embeddings: 'experiments/holist/checkpoints/checkpoint.npy'
path_model_prefix: 'experiments/holist/checkpoints/checkpoint'

tactic_timeout_ms: 5000

max_theorem_parameters: 20

action_generator_options:
#  asm_meson_no_params_only: true
# asm_meson_only: true
  max_theorem_parameters: 20

prover: 'bfs'

bfs_options:
  max_top_suggestions: 5 # k1
  max_successful_branches: 5
  max_explored_nodes: 1

# Standard validation setup

# Specifies which examples to run on. Either "all" or comma separated list
# of library tags. This setting overrides the related setting in the
#library_tags: "complex"
# ProverOptions protobuf.
libraries: "complex"

#splits_to_prove: VALIDATION

# Specifies which examples to run on. Either "all" or comma separated list
# of {"training, "testing" and "validation"} This setting overrides the
# related setting in the ProverOptions protobuf.
splits: 'validation'


# Old tf Flags overrides

# Optional ProverTaskList text protobuf to specify the theorem proving
# tasks. If not supplied then a task list is generated automatically from
# the theorem library. The filtering for training_split is in effect for
# the goals in task_list as well.
task_list: False

# Optional multi-line ProverTask text protobuf or recordio to specify the
# theorem proving tasks. Either this or task list or tasks_by_fingerprint
# must be specified, otherwise the tasks are generated automatically from
# the theorem library. The filtering for training_split is in effect for
# the goals in the read tasks as well.
tasks: False

#'Optional comma-separated list of fingerprints of theorems in the theorem '
#'database. No filtering by training_split in place.

tasks_by_fingerprint: False



# Override the timeout/task specified in the prover options.
timeout_seconds: 20

# Path where proof logs are saved.
output: ${exp_config.checkpoint_dir}/proof_logs.textpbs


# frequency to save logs and eval progress
log_frequency: 10

done_output: ${exp_config.checkpoint_dir}/tasks_done.textpbs