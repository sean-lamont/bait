{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 13:40:24.526201: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-11 13:40:24.545808: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-11 13:40:24.904182: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-11 13:40:25,811] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint 'gen.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage 2, world_size: 1\n",
      "Parsing checkpoint created by deepspeed==0.10.3\n",
      "Reconstructed fp32 state dict with 170 params 299637760 elements\n",
      "Saving fp32 state dict to /tmp/tmpraan0oz9/lightning.cpkt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2023-10-11 13:40:30.507\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36mgenerator.model\u001B[0m:\u001B[36m__init__\u001B[0m:\u001B[36m107\u001B[0m - \u001B[1mWithout retrieval\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing zero checkpoint 'gen.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage 2, world_size: 1\n",
      "Parsing checkpoint created by deepspeed==0.10.3\n",
      "Reconstructed fp32 state dict with 170 params 299637760 elements\n",
      "Saving fp32 state dict to /tmp/tmp58w6dvws/lightning.cpkt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2023-10-11 13:40:36.658\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36mgenerator.model\u001B[0m:\u001B[36m__init__\u001B[0m:\u001B[36m107\u001B[0m - \u001B[1mWithout retrieval\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import ray\n",
    "import torch\n",
    "from lean_dojo import (\n",
    "    Pos,\n",
    "    Dojo,\n",
    "    Theorem,\n",
    "    LeanGitRepo,\n",
    "    ProofFinished,\n",
    "    DojoInitError,\n",
    "    DojoCrashError,\n",
    "    DojoHardTimeoutError,\n",
    ")\n",
    "from lean_dojo.constants import LEAN3_DEPS_DIR, LEAN4_DEPS_DIR\n",
    "from ray.util.actor_pool import ActorPool\n",
    "\n",
    "from common import zip_strict\n",
    "from generator.model import RetrievalAugmentedGenerator\n",
    "from prover.new_search_tree import *\n",
    "\n",
    "ckpt_path = 'gen.ckpt'\n",
    "\n",
    "tac_gen = RetrievalAugmentedGenerator.load(\n",
    "    ckpt_path, device=torch.device(\"cuda\"), freeze=True\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "state_ids = tokenized_state.input_ids.to(device)\n",
    "state_mask = tokenized_state.attention_mask.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "state_ids"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "state_mask"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Generate a single tactic.\n",
    "tactic_ids = tac_gen.generator.generate(tokenized_state.input_ids, max_length=1024)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tactic_ids"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "tactic = tac_gen.tokenizer.decode(tactic_ids[0], skip_special_tokens=True)\n",
    "print(tactic, end=\"\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Generate multiple tactics via beam search.\n",
    "tactic_candidates_ids = tac_gen.generator.generate(\n",
    "    tokenized_state.input_ids,\n",
    "    max_length=1024,\n",
    "    num_beams=4,\n",
    "    length_penalty=0.0,\n",
    "    do_sample=False,\n",
    "    num_return_sequences=4,\n",
    "    early_stopping=False,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tactic_candidates_ids"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "tactic_candidates = tac_gen.tokenizer.batch_decode(\n",
    "    tactic_candidates_ids, skip_special_tokens=True\n",
    ")\n",
    "for tac in tactic_candidates:\n",
    "    print(tac)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model\n",
    "from trl.core import respond_to_batch\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from trl import AutoModelForSeq2SeqLMWithValueHead\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"kaiyuy/leandojo-lean3-tacgen-byt5-small\")       # Or \"lean3\" -> \"lean4\"\n",
    "model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(\"kaiyuy/leandojo-lean3-tacgen-byt5-small\")   # Or \"lean3\" -> \"lean4\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "state = \"n : ℕ\\n⊢ gcd n n = n\"\n",
    "device = 'cuda'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# initialize trainer\n",
    "ppo_config = PPOConfig(\n",
    "    batch_size=1,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_ref = create_reference_model(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "query_tensor = tokenizer.encode(state, return_tensors=\"pt\").to(device)\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "response_tensor = model.generate(query_tensor.to(device))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create a ppo trainer\n",
    "ppo_trainer = PPOTrainer(ppo_config, model, model_ref, tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# define a reward for response\n",
    "# (this could be any reward such as human feedback or output from another model)\n",
    "reward = [torch.tensor(100.0)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(response_tensor[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# train model for one step with ppo\n",
    "train_stats = ppo_trainer.step([query_tensor[0]], [response_tensor[0]], reward)\n",
    "\n",
    "train_stats['ppo/policy/advantages']"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
