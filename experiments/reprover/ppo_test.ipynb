{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import heapq\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import ray\n",
    "import torch\n",
    "from lean_dojo import (\n",
    "    Pos,\n",
    "    Dojo,\n",
    "    Theorem,\n",
    "    LeanGitRepo,\n",
    "    ProofFinished,\n",
    "    DojoInitError,\n",
    "    DojoCrashError,\n",
    "    DojoHardTimeoutError,\n",
    ")\n",
    "from lean_dojo.constants import LEAN3_DEPS_DIR, LEAN4_DEPS_DIR\n",
    "from ray.util.actor_pool import ActorPool\n",
    "\n",
    "from common import zip_strict\n",
    "from generator.model import RetrievalAugmentedGenerator\n",
    "from prover.new_search_tree import *\n",
    "\n",
    "ckpt_path = 'gen.ckpt'\n",
    "\n",
    "tac_gen = RetrievalAugmentedGenerator.load(\n",
    "    ckpt_path, device=torch.device(\"cuda\"), freeze=True\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tac_gen"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_state = tac_gen.tokenizer(state, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "state_ids = tokenized_state.input_ids.to(device)\n",
    "state_mask = tokenized_state.attention_mask.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "state_ids"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "state_mask"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Generate a single tactic.\n",
    "tactic_ids = tac_gen.generator.generate(tokenized_state.input_ids, max_length=1024)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tactic_ids"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "tactic = tac_gen.tokenizer.decode(tactic_ids[0], skip_special_tokens=True)\n",
    "print(tactic, end=\"\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Generate multiple tactics via beam search.\n",
    "tactic_candidates_ids = tac_gen.generator.generate(\n",
    "    tokenized_state.input_ids,\n",
    "    max_length=1024,\n",
    "    num_beams=4,\n",
    "    length_penalty=0.0,\n",
    "    do_sample=False,\n",
    "    num_return_sequences=4,\n",
    "    early_stopping=False,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tactic_candidates_ids"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "tactic_candidates = tac_gen.tokenizer.batch_decode(\n",
    "    tactic_candidates_ids, skip_special_tokens=True\n",
    ")\n",
    "for tac in tactic_candidates:\n",
    "    print(tac)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model\n",
    "from trl.core import respond_to_batch\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from trl import AutoModelForSeq2SeqLMWithValueHead\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"kaiyuy/leandojo-lean3-tacgen-byt5-small\")       # Or \"lean3\" -> \"lean4\"\n",
    "model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(\"kaiyuy/leandojo-lean3-tacgen-byt5-small\")   # Or \"lean3\" -> \"lean4\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "state = \"n : ℕ\\n⊢ gcd n n = n\"\n",
    "device = 'cuda'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# initialize trainer\n",
    "ppo_config = PPOConfig(\n",
    "    batch_size=1,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_ref = create_reference_model(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "query_tensor = tokenizer.encode(state, return_tensors=\"pt\").to(device)\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "response_tensor = model.generate(query_tensor.to(device))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create a ppo trainer\n",
    "ppo_trainer = PPOTrainer(ppo_config, model, model_ref, tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# define a reward for response\n",
    "# (this could be any reward such as human feedback or output from another model)\n",
    "reward = [torch.tensor(100.0)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(response_tensor[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# train model for one step with ppo\n",
    "train_stats = ppo_trainer.step([query_tensor[0]], [response_tensor[0]], reward)\n",
    "\n",
    "train_stats['ppo/policy/advantages']"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
