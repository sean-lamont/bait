{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import heapq\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import ray\n",
    "import torch\n",
    "from lean_dojo import (\n",
    "    Pos,\n",
    "    Dojo,\n",
    "    Theorem,\n",
    "    LeanGitRepo,\n",
    "    ProofFinished,\n",
    "    DojoInitError,\n",
    "    DojoCrashError,\n",
    "    DojoHardTimeoutError,\n",
    ")\n",
    "from lean_dojo.constants import LEAN3_DEPS_DIR, LEAN4_DEPS_DIR\n",
    "from ray.util.actor_pool import ActorPool\n",
    "\n",
    "from common import zip_strict\n",
    "from generator.model import RetrievalAugmentedGenerator\n",
    "from prover.new_search_tree import *\n",
    "\n",
    "ckpt_path = 'gen.ckpt'\n",
    "\n",
    "tac_gen = RetrievalAugmentedGenerator.load(\n",
    "    ckpt_path, device=torch.device(\"cuda\"), freeze=True\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# new_tok = ['<critic>', '<provable>', '<unprovable>']\n",
    "# tac_gen.tokenizer.add_tokens(new_tok)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(tac_gen.tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# tac_gen.generator.resize_token_embeddings(len(tac_gen.tokenizer), pad_to_multiple_of=1)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tac_gen.generator"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# use existing sentinel/extra tokens for goal task\n",
    "critic_tok = '<extra_id_0>'\n",
    "provable_tok = '<extra_id_1>'\n",
    "unprovable_tok = '<extra_id_2>'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "critic_id = tac_gen.tokenizer.encode(critic_tok)[0]\n",
    "provable_id = tac_gen.tokenizer.encode(provable_tok)[0]\n",
    "unprovable_id = tac_gen.tokenizer.encode(unprovable_tok)[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "state = critic_tok + 'C : Type u₁,\\n_inst_1 : category C,\\nD : Type u₂,\\n_inst_2 : category D,\\nF : C ⥤ D,\\nW X Y Z : C,\\nf : W ⟶ X,\\ng : W ⟶ Y,\\nh : X ⟶ Z,\\ni : Y ⟶ Z,\\n_inst_3 : reflects_colimit (span f g) F,\\ne : f ≫ h = g ≫ i,\\nH : is_pushout (F.map f) (F.map g) (F.map h) (F.map i)\\n⊢ comm_sq f g h i'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenized_state = tac_gen.tokenizer(\n",
    "    state,\n",
    "    padding=\"longest\",\n",
    "    max_length=1024,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenized_state"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tac_gen.tokenizer.decode(tokenized_state['input_ids'][0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ids = tokenized_state.input_ids.cuda()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# restrict output to just be provable and unprovable\n",
    "bad_ids = [[i] for i in range(len(tac_gen.tokenizer)) if (i != provable_id and i != unprovable_id)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bad_ids"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tactic_ids = tac_gen.generator.generate(ids,\n",
    "                                        max_new_tokens=2,\n",
    "                                        bad_words_ids=bad_ids,\n",
    "                                        return_dict_in_generate=True,\n",
    "                                        output_scores=True,)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tactic_ids.scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "provable_score = torch.exp(tactic_ids.scores[0][0][provable_id])\n",
    "unprovable_score = torch.exp(tactic_ids.scores[0][0][unprovable_id])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tactic_ids.sequences[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tac_gen.tokenizer.batch_decode(tactic_ids[0], skip_special_tokens=True)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "provable_score = tactic_ids.scores[0][0][provable_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unprovable_score = tactic_ids.scores[0][0][unprovable_id]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from goal_model.datamodule import GoalDataModule\n",
    "\n",
    "critic_tok = '<extra_id_0>'\n",
    "provable_tok = '<extra_id_1>'\n",
    "unprovable_tok = '<extra_id_2>'\n",
    "\n",
    "module = GoalDataModule(data_path='goal_data.pk',\n",
    "                        max_seq_len=1024,\n",
    "                        batch_size=8,\n",
    "                        model_name=\"kaiyuy/leandojo-lean3-tacgen-byt5-small\",\n",
    "                        critic_tok=critic_tok,\n",
    "                        provable_tok=provable_tok,\n",
    "                        unprovable_tok=unprovable_tok,\n",
    "                        num_workers=4,\n",
    "                        val_data_path=None,\n",
    "                        eval_batch_size=8,\n",
    "                        visit_threshold=256\n",
    "                        )\n",
    "\n",
    "\n",
    "\n",
    "module.setup('fit')\n",
    "loader = module.train_dataloader()\n",
    "print (next(iter(loader)))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len([d for d in module.ds_train.data if d['proved'] == 1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('lightning_logs/version_18/events.out.tfevents.1697004955.pc.48076.0', 'rb') as f:\n",
    "    res = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
