{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"train_test_data.pk\", \"rb\") as f:\n",
    "    train, val, test, enc_nodes = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#data in polished format with (goal, premise)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#process data to remove variable/ function variables as in graph\n",
    "\n",
    "from ast_def import *\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"include_probability.json\") as f:\n",
    "    db = json.load(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokens = list(\n",
    "        set([token.value for polished_goal in db.keys() for token in polished_to_tokens_2(polished_goal)]))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torchtext.data import get_tokenizer\n",
    "import re\n",
    "\n",
    "# def tokenizer(inp_str): ## This method is one way of creating tokenizer that looks for word tokens\n",
    "#     return re.findall(r\"\\w+\", inp_str)\n",
    "#\n",
    "# tokenizer = get_tokenizer(\"basic_english\") ## We'll use tokenizer available from PyTorch\n",
    "#\n",
    "\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "def build_vocab(l):\n",
    "    for token in l:\n",
    "        yield [token]\n",
    "\n",
    "vocab = build_vocab_from_iterator(build_vocab(tokens), specials=[\"<UNK>\"], min_freq=0)\n",
    "vocab.set_default_index(vocab[\"<UNK>\"])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# len(vocab)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_seq = []\n",
    "\n",
    "max_len = 1024\n",
    "\n",
    "\n",
    "for i, (goal, premise, y) in enumerate(train):\n",
    "    train_seq.append(([i.value for i in polished_to_tokens_2(goal)], [i.value for i in polished_to_tokens_2(premise)], y))\n",
    "\n",
    "val_seq = []\n",
    "for i, (goal, premise, y) in enumerate(val):\n",
    "    val_seq.append(([i.value for i in polished_to_tokens_2(goal)], [i.value for i in polished_to_tokens_2(premise)], y))\n",
    "\n",
    "test_seq = []\n",
    "for i, (goal, premise, y) in enumerate(test):\n",
    "    test_seq.append(([i.value for i in polished_to_tokens_2(goal)], [i.value for i in polished_to_tokens_2(premise)], y))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# goals, premises, targets = list(zip(*train))\n",
    "\n",
    "train_goals = []\n",
    "train_premises = []\n",
    "train_targets = []\n",
    "\n",
    "for goal, premise, y in train_seq:\n",
    "    train_goals.append(goal)\n",
    "    train_premises.append(premise)\n",
    "    train_targets.append(y)\n",
    "\n",
    "\n",
    "val_goals = []\n",
    "val_premises = []\n",
    "val_targets = []\n",
    "\n",
    "for goal, premise, y in val_seq:\n",
    "    val_goals.append(goal)\n",
    "    val_premises.append(premise)\n",
    "    val_targets.append(y)\n",
    "\n",
    "test_goals = []\n",
    "test_premises = []\n",
    "test_targets = []\n",
    "\n",
    "for goal, premise, y in test_seq:\n",
    "    test_goals.append(goal)\n",
    "    test_premises.append(premise)\n",
    "    test_targets.append(y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def vectorise(goal_list, premise_list, target_list, max_len=1024):\n",
    "    idx_list = [vocab(toks) for toks in goal_list]\n",
    "    X_G = [sample+([0]* (max_len-len(sample))) if len(sample)<max_len else sample[:max_len] for sample in idx_list]\n",
    "    idx_list = [vocab(toks) for toks in premise_list]\n",
    "    X_P = [sample+([0]* (max_len-len(sample))) if len(sample)<max_len else sample[:max_len] for sample in idx_list]\n",
    "    return torch.tensor(X_G, dtype=torch.int32), torch.tensor(X_P, dtype=torch.int32), torch.tensor(target_list, dtype=torch.long)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataset = vectorise(train_goals, train_premises, train_targets)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "val_data = vectorise(val_goals, val_premises, val_targets)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# G,P,Y = train_dataset\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# dataset = TensorDataset(G,P,Y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# batch_size = 50"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# loader = DataLoader(dataset, batch_size=batch_size)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# torch transformer code\n",
    "from typing import Tuple\n",
    "\n",
    "from torch.utils.data import dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "\n",
    "        # self.initial_encoder = inner_embedding_network.F_x_module_(ntoken, d_model)\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        return output\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def gen_embedding(model, input, src_mask):\n",
    "    out = model(input, src_mask)\n",
    "    out = torch.transpose(out,1,2)\n",
    "    gmp = nn.MaxPool1d(1024, stride=1)\n",
    "\n",
    "    return torch.cat([gmp(out).squeeze(-1), torch.sum(out,dim=2)], dim = 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# gen_embedding(model, )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model = TransformerEmbedding(ntoken=len(vocab), d_model=128, nhead=8, d_hid=128, nlayers=8).to(device)\n",
    "# input_test = g.to(device)\n",
    "# src_mask = generate_square_subsequent_mask(batch_size).to(device)\n",
    "# out = model(input_test, src_mask)\n",
    "# out = torch.transpose(out,1,2)\n",
    "# out.shape\n",
    "# gmp = nn.MaxPool1d(1024, stride=1)\n",
    "# gap = nn.AvgPool1d(1024, stride=1)\n",
    "# out = torch.transpose(out, 0,2)\n",
    "# s = torch.sum(out,dim=2)\n",
    "# s.shape\n",
    "# gmp(out).squeeze(-1).shape\n",
    "#sum and max pooling, since sequences are padded which effects avg pool calculation\n",
    "\n",
    "# embedding = torch.cat([gmp(out).squeeze(-1), torch.sum(out,dim=2)], dim = 1).shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train loop\n",
    "# de\n",
    "# for j in range(num_epochs):\n",
    "#\n",
    "#     for batch_idx, (g,p,y) in enumerate(loader):\n",
    "        # print (g.shape,p.shape,y.shape)\n",
    "\n",
    "        # break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import inner_embedding_network\n",
    "\n",
    "def binary_loss(preds, targets):\n",
    "    return -1. * torch.sum(targets * torch.log(preds) + (1 - targets) * torch.log((1. - preds)))\n",
    "\n",
    "\n",
    "#run_edges(1e-3, 0, 20, 1024, 64, 0, False)\n",
    "#run_2(1e-3, 0, 20, 1024, 64, 4, False)\n",
    "\n",
    "def accuracy_transformer(model_1, model_2,batch, fc):\n",
    "    g,p,y = batch\n",
    "    batch_size = len(g)\n",
    "    src_mask = generate_square_subsequent_mask(batch_size).to(device)\n",
    "    embedding_1 = gen_embedding(model_1, g, src_mask)\n",
    "    embedding_2 = gen_embedding(model_2, p, src_mask)\n",
    "\n",
    "    preds = fc(torch.cat([embedding_1, embedding_2], axis=1))\n",
    "\n",
    "    preds = torch.flatten(preds)\n",
    "\n",
    "    preds = (preds>0.5).long()\n",
    "\n",
    "    return torch.sum(preds == torch.LongTensor(y).to(device)) / len(y)\n",
    "\n",
    "def run_transformer_pretrain(step_size, decay_rate, num_epochs, batch_size, embedding_dim, save=False):\n",
    "\n",
    "    # loader = DataLoader(new_train, batch_size=batch_size, follow_batch=['x_s', 'x_t'])\n",
    "\n",
    "    # val_loader = iter(DataLoader(new_val, batch_size=2048, follow_batch=['x_s', 'x_t']))\n",
    "\n",
    "    G,P,Y = train_dataset\n",
    "\n",
    "    dataset = TensorDataset(G,P,Y)\n",
    "    # batch_size = 50\n",
    "    loader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "    V_G, V_P, V_Y = val_data\n",
    "    val_dataset = TensorDataset(V_G, V_P, V_Y)\n",
    "\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model_1 = TransformerEmbedding(ntoken=len(vocab), d_model=128, nhead=8, d_hid=128, nlayers=8).to(device)\n",
    "    model_2 = TransformerEmbedding(ntoken=len(vocab), d_model=128, nhead=8, d_hid=128, nlayers=8).to(device)\n",
    "    fc = inner_embedding_network.F_c_module_(embedding_dim * 8).to(device)\n",
    "\n",
    "    op_1 =torch.optim.Adam(model_1.parameters(), lr=step_size)\n",
    "    op_2 =torch.optim.Adam(model_2.parameters(), lr=step_size)\n",
    "    op_fc =torch.optim.Adam(fc.parameters(), lr=step_size)\n",
    "\n",
    "    training_losses = []\n",
    "\n",
    "    val_losses = []\n",
    "    best_acc = 0.\n",
    "\n",
    "    src_mask = generate_square_subsequent_mask(batch_size).to(device)\n",
    "\n",
    "    for j in range(num_epochs):\n",
    "        print (f\"Epoch: {j}\")\n",
    "\n",
    "        for batch_idx, (g,p,y) in enumerate(loader):\n",
    "            # op_enc.zero_grad()\n",
    "            op_1.zero_grad()\n",
    "            op_2.zero_grad()\n",
    "            op_fc.zero_grad()\n",
    "\n",
    "\n",
    "            # embedding_1 = model_1(g, src_mask)\n",
    "            # embedding_2 = model_2(g, src_mask)\n",
    "\n",
    "            embedding_1 = gen_embedding(model_1, g.to(device), src_mask)\n",
    "            embedding_2 = gen_embedding(model_2, p.to(device), src_mask)\n",
    "\n",
    "            preds = fc(torch.cat([embedding_1, embedding_2], axis=1))\n",
    "\n",
    "            eps = 1e-6\n",
    "\n",
    "            preds = torch.clip(preds, eps, 1 - eps)\n",
    "\n",
    "            loss = binary_loss(torch.flatten(preds), torch.LongTensor(y).to(device))\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            op_1.step()\n",
    "            op_2.step()\n",
    "            op_fc.step()\n",
    "\n",
    "            training_losses.append(loss.detach() / batch_size)\n",
    "\n",
    "            if i % 100 == 0:\n",
    "\n",
    "                validation_loss = accuracy_transformer(model_1, model_2, next(val_loader), fc)#, fp, fi, fo, fx, fc,conv1,conv2, graph_iterations)\n",
    "\n",
    "                val_losses.append((validation_loss.detach(), j, i))\n",
    "\n",
    "                val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "                print (\"Curr training loss avg: {}\".format(sum(training_losses[-100:]) / len(training_losses[-100:])))\n",
    "\n",
    "                print (\"Val acc: {}\".format(validation_loss.detach()))\n",
    "\n",
    "                # if validation_loss > best_acc:\n",
    "                #     best_acc = validation_loss\n",
    "                #     print (f\"New best validation accuracy: {best_acc}\")\n",
    "                #     only save encoder if best accuracy so far\n",
    "                    # if save == True:\n",
    "                    #     torch.save(model_1, \"model_checkpoints/tranformer_encoder_latest_goal\")\n",
    "                    #     torch.save(model_2, \"model_checkpoints/tranformer_encoder_latest_premise\")\n",
    "\n",
    "    print (f\"Best validation accuracy: {best_acc}\")\n",
    "\n",
    "    return training_losses, val_losses\n",
    "\n",
    "run_transformer_pretrain(1e-3, 0, 40, 128, 64, 2)#, save=True)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Doesn't make sense to process VAR as with graph so leave tokens for now\n",
    "# def seq_to_tokens(seq):\n",
    "#\n",
    "#     seq_list = [i.value for i in polished_to_tokens_2(seq)]\n",
    "#     node_list = nodes_list(goal_to_graph_labelled(seq), result=[])\n",
    "#\n",
    "#     var_dict = {}\n",
    "#     for node in node_list:\n",
    "#         if node.node.value[0] == 'V':\n",
    "#             if node.children != []:\n",
    "#                 var_dict[node.node.value] = \"VARFUNC\"\n",
    "#             elif node.node.value not in var_dict.keys():\n",
    "#                 var_dict[node.node.value] = \"VAR\"\n",
    "#         else:\n",
    "#             var_dict[node.node.value] = node.node.value\n",
    "#     # print (list(var_dict.keys()))\n",
    "#     # print ([i.node.value for i in node_list])\n",
    "#     # print (seq)\n",
    "#     # print (seq_list)\n",
    "#     # print ([var_dict[i] for i in seq_list])\n",
    "#     return [var_dict[i] for i in seq_list]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train_seq = []\n",
    "# # exps = []\n",
    "#\n",
    "# # Some cases where token isn't properly processed.. 540/294577 when tested so ignore for now\n",
    "#\n",
    "# # count = 0\n",
    "# for i, (goal, premise, label) in enumerate(train):\n",
    "#     try:\n",
    "#         train_seq.append((seq_to_tokens(goal), seq_to_tokens(premise), label))\n",
    "#     except Exception as e:\n",
    "#         # exps.append(goal)\n",
    "#         # exps.append(premise)\n",
    "#         # count += 1\n",
    "#         continue"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# val_seq =[]\n",
    "# for i, (goal, premise, label) in enumerate(val):\n",
    "#     try:\n",
    "#         val_seq.append((seq_to_tokens(goal), seq_to_tokens(premise), label))\n",
    "#     except Exception as e:\n",
    "#         # exps.append(goal)\n",
    "#         # exps.append(premise)\n",
    "#         # count += 1\n",
    "#         continue\n",
    "#\n",
    "# test_seq = []\n",
    "# # for (goal, premise, label) in test:\n",
    "# #     test_seq.append((seq_to_tokens(goal), seq_to_tokens(premise), label))\n",
    "# for i, (goal, premise, label) in enumerate(test):\n",
    "#     try:\n",
    "#         test_seq.append((seq_to_tokens(goal), seq_to_tokens(premise), label))\n",
    "#     except Exception as e:\n",
    "#         # exps.append(goal)\n",
    "#         # exps.append(premise)\n",
    "#         # count += 1\n",
    "#         continue\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train_seq[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# db[seq]"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "torch_env",
   "language": "python",
   "display_name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
