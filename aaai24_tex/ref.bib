@inproceedings{wu_int_2020,
    title = {{INT}: {An} {Inequality} {Benchmark} for {Evaluating} {Generalization} in {Theorem} {Proving}},
    copyright = {arXiv.org perpetual, non-exclusive license},
    shorttitle = {{INT}},
    url = {https://arxiv.org/abs/2007.02924},
    doi = {10.48550/ARXIV.2007.02924},
    abstract = {In learning-assisted theorem proving, one of the most critical challenges is to generalize to theorems unlike those seen at training time. In this paper, we introduce INT, an INequality Theorem proving benchmark, specifically designed to test agents' generalization ability. INT is based on a procedure for generating theorems and proofs; this procedure's knobs allow us to measure 6 different types of generalization, each reflecting a distinct challenge characteristic to automated theorem proving. In addition, unlike prior benchmarks for learning-assisted theorem proving, INT provides a lightweight and user-friendly theorem proving environment with fast simulations, conducive to performing learning-based and search-based research. We introduce learning-based baselines and evaluate them across 6 dimensions of generalization with the benchmark. We then evaluate the same agents augmented with Monte Carlo Tree Search (MCTS) at test time, and show that MCTS can help to prove new theorems.},
    urldate = {2023-07-25},
    booktitle = {ICLR},
    author = {Wu, Yuhuai and Jiang, Albert Qiaochu and Ba, Jimmy and Grosse, Roger},
    year = {2021},
    keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, Logic in Computer Science (cs.LO), Machine Learning (cs.LG), Machine Learning (stat.ML)},
}

@inproceedings{bansal_holist_2019,
    title = {Holist: {An} environment for machine learning of higher order logic theorem proving},
    booktitle = {International {Conference} on {Machine} {Learning}},
    publisher = {PMLR},
    author = {Bansal, Kshitij and Loos, Sarah and Rabe, Markus and Szegedy, Christian and Wilcox, Stewart},
    year = {2019},
    pages = {454--463},
}

@inproceedings{gauthier_sharing_2015,
    title = {Sharing HOL4 and HOL Light Proof Knowledge},
    author = {Thibault Gauthier and C. Kaliszyk},
    booktitle = {Logic Programming and Automated Reasoning},
    year = {2015},
    url = {https://api.semanticscholar.org/CorpusID:14375140}
}


@article{zhang_graph_2019,
    title = {Graph convolutional networks: a comprehensive review},
    volume = {6},
    issn = {2197-4314},
    shorttitle = {Graph convolutional networks},
    url = {https://computationalsocialnetworks.springeropen.com/articles/10.1186/s40649-019-0069-y},
    doi = {10.1186/s40649-019-0069-y},
    abstract = {Abstract
 Graphs naturally appear in numerous application domains, ranging from social analysis, bioinformatics to computer vision. The unique capability of graphs enables capturing the structural relations among data, and thus allows to harvest more insights compared to analyzing data in isolation. However, it is often very challenging to solve the learning problems on graphs, because (1) many types of data are not originally structured as graphs, such as images and text data, and (2) for graph-structured data, the underlying connectivity patterns are often complex and diverse. On the other hand, the representation learning has achieved great successes in many areas. Thereby, a potential solution is to learn the representation of graphs in a low-dimensional Euclidean space, such that the graph properties can be preserved. Although tremendous efforts have been made to address the graph representation learning problem, many of them still suffer from their shallow learning mechanisms. Deep learning models on graphs (e.g., graph neural networks) have recently emerged in machine learning and other related areas, and demonstrated the superior performance in various problems. In this survey, despite numerous types of graph neural networks, we conduct a comprehensive review specifically on the emerging field of graph convolutional networks, which is one of the most prominent graph deep learning models. First, we group the existing graph convolutional network models into two categories based on the types of convolutions and highlight some graph convolutional network models in details. Then, we categorize different graph convolutional networks according to the areas of their applications. Finally, we present several open challenges in this area and discuss potential directions for future research.},
    language = {en},
    number = {1},
    urldate = {2023-08-14},
    journal = {Computational Social Networks},
    author = {Zhang, Si and Tong, Hanghang and Xu, Jiejun and Maciejewski, Ross},
    month = dec,
    year = {2019},
    pages = {11},
}

@inproceedings{vaswani_attention_2017,
    author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
    pages = {},
    publisher = {Curran Associates, Inc.},
    title = {Attention is All you Need},
    url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
    volume = {30},
    year = {2017}
}

@misc{yadan_hydra_2019,
    title = {Hydra - {A} framework for elegantly configuring complex applications},
    url = {https://github.com/facebookresearch/hydra},
    author = {Yadan, Omry},
    year = {2019},
    note = {Software available at https://github.com/facebookresearch/hydra}
}

@inproceedings{
han_proof_2021,
    title = {Proof Artifact Co-Training for Theorem Proving with Language Models},
    author = {Jesse Michael Han and Jason Rute and Yuhuai Wu and Edward Ayers and Stanislas Polu},
    booktitle = {International Conference on Learning Representations},
    year = {2022},
    url = {https://openreview.net/forum?id=rpxJc9j04U}
}

@article{poesia_peano_2023,
    title = {Peano: learning formal mathematical reasoning},
    volume = {381},
    issn = {1364-503X, 1471-2962},
    shorttitle = {Peano},
    url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2022.0044},
    doi = {10.1098/rsta.2022.0044},
    abstract = {General mathematical reasoning is computationally undecidable, but humans routinely solve new problems. Moreover, discoveries developed over centuries are taught to subsequent generations quickly. What structure enables this, and how might that inform automated mathematical reasoning? We posit that central to both puzzles is the structure of procedural abstractions underlying mathematics. We explore this idea in a case study on five sections of beginning algebra on the Khan Academy platform. To define a computational foundation, we introduce Peano, a theorem-proving environment where the set of valid actions at any point is finite. We use Peano to formalize introductory algebra problems and axioms, obtaining well-defined search problems. We observe existing reinforcement learning methods for symbolic reasoning to be insufficient to solve harder problems. Adding the ability to induce reusable abstractions (‘tactics’) from its own solutions allows an agent to make steady progress, solving all problems. Furthermore, these abstractions induce an order to the problems, seen at random during training. The recovered order has significant agreement with the expert-designed Khan Academy curriculum, and second-generation agents trained on the recovered curriculum learn significantly faster. These results illustrate the synergistic role of abstractions and curricula in the cultural transmission of mathematics.
 This article is part of a discussion meeting issue ‘Cognitive artificial intelligence’.},
    language = {en},
    number = {2251},
    urldate = {2023-08-03},
    journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
    author = {Poesia, Gabriel and Goodman, Noah D.},
    month = jul,
    year = {2023},
    pages = {20220044},
}


@article{polu_generative_2020,
    title = {Generative {Language} {Modeling} for {Automated} {Theorem} {Proving}},
    copyright = {arXiv.org perpetual, non-exclusive license},
    url = {https://arxiv.org/abs/2009.03393},
    doi = {10.48550/ARXIV.2009.03393},
    abstract = {We explore the application of transformer-based language models to automated theorem proving. This work is motivated by the possibility that a major limitation of automated theorem provers compared to humans -- the generation of original mathematical terms -- might be addressable via generation from language models. We present an automated prover and proof assistant, GPT-f, for the Metamath formalization language, and analyze its performance. GPT-f found new short proofs that were accepted into the main Metamath library, which is to our knowledge, the first time a deep-learning based system has contributed proofs that were adopted by a formal mathematics community.},
    urldate = {2023-08-03},
    author = {Polu, Stanislas and Sutskever, Ilya},
    year = {2020},
    note = {arXiv:2009.03393},
    keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML)},
}

@inproceedings{
li_isarstep_2020,
    title = {IsarStep: a Benchmark for High-level Mathematical Reasoning},
    author = {Wenda Li and Lei Yu and Yuhuai Wu and Lawrence C. Paulson},
    booktitle = {International Conference on Learning Representations},
    year = {2021},
    url = {https://openreview.net/forum?id=Pzj6fzU6wkj}
}


@inproceedings{yang_learning_2019,
    title = "Learning to prove theorems via interacting with proof assistants",
    abstract = "Humans prove theorems by relying on substantial high-level reasoning and problem-specific insights. Proof assistants offer a formalism that resembles human mathematical reasoning, representing theorems in higher-order logic and proofs as high-level tactics. However, human experts have to construct proofs manually by entering tactics into the proof assistant. In this paper, we study the problem of using machine learning to automate the interaction with proof assistants. We construct CoqGym, a large-scale dataset and learning environment containing 71K human-written proofs from 123 projects developed with the Coq proof assistant. We develop ASTactic, a deep learning-based model that generates tactics as programs in the form of abstract syntax trees (ASTs). Experiments show that ASTactic trained on CoqGym can generate effective tactics and can be used to prove new theorems not previously provable by automated methods. Code is available at h t t p s: / / g i t h u b. com/ p r i n c e t o n - v l / C o q G y m.",
    author = "Kaiyu Yang and Jia Deng",
    year = "2019",
    month = jan,
    day = "1",
    language = "English (US)",
    series = "36th International Conference on Machine Learning, ICML 2019",
    publisher = "International Machine Learning Society (IMLS)",
    pages = "12079--12094",
    booktitle = "36th International Conference on Machine Learning, ICML 2019",
    note = "36th International Conference on Machine Learning, ICML 2019 ; Conference date: 09-06-2019 Through 15-06-2019",

}

@misc{wandb,
    title = {Experiment Tracking with Weights and Biases},
    year = {2020},
    note = {Software available from wandb.com},
    url = {https://www.wandb.com/},
    author = {Biewald, Lukas},
}

@book{powell_reinforcement_2022,
    title = {Reinforcement {Learning} and {Stochastic} {Optimization}: {A} {Unified} {Framework} for {Sequential} {Decisions}},
    isbn = {978-1-119-81505-1},
    shorttitle = {Reinforcement {Learning} and {Stochastic} {Optimization}},
    abstract = {REINFORCEMENT LEARNING AND STOCHASTIC OPTIMIZATION Clearing the jungle of stochastic optimization Sequential decision problems, which consist of “decision, information, decision, information,” are ubiquitous, spanning virtually every human activity ranging from business applications, health (personal and public health, and medical decision making), energy, the sciences, all fields of engineering, finance, and e-commerce. The diversity of applications attracted the attention of at least 15 distinct fields of research, using eight distinct notational systems which produced a vast array of analytical tools. A byproduct is that powerful tools developed in one community may be unknown to other communities. Reinforcement Learning and Stochastic Optimization offers a single canonical framework that can model any sequential decision problem using five core components: state variables, decision variables, exogenous information variables, transition function, and objective function. This book highlights twelve types of uncertainty that might enter any model and pulls together the diverse set of methods for making decisions, known as policies, into four fundamental classes that span every method suggested in the academic literature or used in practice. Reinforcement Learning and Stochastic Optimization is the first book to provide a balanced treatment of the different methods for modeling and solving sequential decision problems, following the style used by most books on machine learning, optimization, and simulation. The presentation is designed for readers with a course in probability and statistics, and an interest in modeling and applications. Linear programming is occasionally used for specific problem classes. The book is designed for readers who are new to the field, as well as those with some background in optimization under uncertainty. Throughout this book, readers will find references to over 100 different applications, spanning pure learning problems, dynamic resource allocation problems, general state-dependent problems, and hybrid learning/resource allocation problems such as those that arose in the COVID pandemic. There are 370 exercises, organized into seven groups, ranging from review questions, modeling, computation, problem solving, theory, programming exercises and a "diary problem" that a reader chooses at the beginning of the book, and which is used as a basis for questions throughout the rest of the book.},
    language = {en},
    publisher = {John Wiley \& Sons},
    author = {Powell, Warren B.},
    month = apr,
    year = {2022},
    note = {Google-Books-ID: 6ahsEAAAQBAJ},
    keywords = {Computers / Artificial Intelligence / General, Mathematics / General, Mathematics / Linear \& Nonlinear Programming, Mathematics / Optimization},
}

@inproceedings{
evans_can_2018,
    title = {Can Neural Networks Understand Logical Entailment?},
    author = {Richard Evans and David Saxton and David Amos and Pushmeet Kohli and Edward Grefenstette},
    booktitle = {International Conference on Learning Representations},
    year = {2018},
    url = {https://openreview.net/forum?id=SkZxCk-0Z},
}

@book{sutton_reinforcement_2018,
    address = {Cambridge, MA, USA},
    title = {Reinforcement {Learning}: {An} {Introduction}},
    isbn = {978-0-262-03924-6},
    shorttitle = {Reinforcement {Learning}},
    abstract = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence. Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics. Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.},
    publisher = {A Bradford Book},
    author = {Sutton, Richard S. and Barto, Andrew G.},
    month = oct,
    year = {2018},
}


@inproceedings{yang_leandojo_2023,
    title = {{LeanDojo}: Theorem Proving with Retrieval-Augmented Language Models},
    author = {Yang, Kaiyu and Swope, Aidan and Gu, Alex and Chalamala, Rahul and Song, Peiyang and Yu, Shixing and Godil, Saad and Prenger, Ryan and Anandkumar, Anima},
    booktitle = {Neural Information Processing Systems (NeurIPS)},
    year = {2023}
}


@inproceedings{
huang_gamepad_2018,
    title = {GamePad: A Learning Environment for Theorem Proving},
    author = {Daniel Huang and Prafulla Dhariwal and Dawn Song and Ilya Sutskever},
    booktitle = {International Conference on Learning Representations},
    year = {2019},
    url = {https://openreview.net/forum?id=r1xwKoR9Y7},
}


@inproceedings{
zheng_minif2f_2021,
    title = {miniF2F: a cross-system benchmark for formal Olympiad-level mathematics},
    author = {Kunhao Zheng and Jesse Michael Han and Stanislas Polu},
    booktitle = {International Conference on Learning Representations},
    year = {2022},
    url = {https://openreview.net/forum?id=9ZPegFuFTFv}
}

@inproceedings{mishra_lila_2022,
    title = "{LILA}: A Unified Benchmark for Mathematical Reasoning",
    author = "Mishra, Swaroop  and
      Finlayson, Matthew  and
      Lu, Pan  and
      Tang, Leonard  and
      Welleck, Sean  and
      Baral, Chitta  and
      Rajpurohit, Tanmay  and
      Tafjord, Oyvind  and
      Sabharwal, Ashish  and
      Clark, Peter  and
      Kalyan, Ashwin",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.392",
    doi = "10.18653/v1/2022.emnlp-main.392",
    pages = "5807--5832",
    abstract = "Mathematical reasoning skills are essential for general-purpose intelligentsystems to perform tasks from grocery shopping to climate modeling. Towards evaluating and improving AI systems in this domain, we proposeLILA, a unified mathematical reasoning benchmark consisting of 23 diversetasks along four dimensions:(i) mathematical abilities e.g., arithmetic, calculus (ii) language format e.g., question-answering, fill-in-the-blanks (iii) language diversity e.g., no language, simple language (iv) external knowledge e.g., commonsense, physics. We construct our benchmark by extending 20 datasets benchmark by collecting task instructions and solutions in the form of Python programs,thereby obtaining explainable solutions in addition to the correct answer. We additionally introduce two evaluation datasets to measure out-of-distribution performance and robustness to language perturbation. Finally, we introduce BHASKARA,a general-purpose mathematical reasoning model trained on LILA. Importantly, we find that multi-tasking leads to significant improvements (average relative improvement of 21.83{\%} F1 score vs. single-task models),while the best performing model only obtains 60.40{\%},indicating the room for improvement in general mathematical reasoning and understanding.",
}


@article{whalen_holophrasm_2016,
    title = {Holophrasm: a neural Automated Theorem Prover for higher-order logic},
    author = {Daniel Whalen},
    year = {2016},
    note = {arXiv:1608.02644},
    url = {https://api.semanticscholar.org/CorpusID:16000088}
}

journal={ArXiv},
volume={abs/1608.02644},

@inproceedings{wu_tacticzero_2021,
    title = {{TacticZero}: {Learning} to {Prove} {Theorems} from {Scratch} with {Deep} {Reinforcement} {Learning}},
    volume = {34},
    shorttitle = {{TacticZero}},
    url = {https://proceedings.neurips.cc/paper/2021/hash/4dea382d82666332fb564f2e711cbc71-Abstract.html},
    abstract = {We propose a novel approach to interactive theorem-proving (ITP) using deep reinforcement learning. The proposed framework is able to learn proof search strategies as well as tactic and arguments prediction in an end-to-end manner. We formulate the process of ITP as a Markov decision process (MDP) in which each state represents a set of potential derivation paths. This structure allows us to introduce a novel backtracking mechanism which enables the agent to efficiently discard (predicted) dead-end derivations and restart the derivation from promising alternatives. We implement the framework in the HOL theorem prover. Experimental results show that the framework using learned search strategies outperforms existing automated theorem provers (i.e., hammers) available in HOL when evaluated on unseen problems. We further elaborate the role of key components of the framework using ablation studies.},
    urldate = {2023-07-25},
    booktitle = {NeurIPS},
    author = {Wu, Minchao and Norrish, Michael and Walder, Christian and Dezfouli, Amir},
    year = {2021},
}

@article{lan_mwptoolkit_2022,
    title = {{MWPToolkit}: {An} {Open}-{Source} {Framework} for {Deep} {Learning}-{Based} {Math} {Word} {Problem} {Solvers}},
    volume = {36},
    copyright = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
    issn = {2374-3468},
    shorttitle = {{MWPToolkit}},
    url = {https://ojs.aaai.org/index.php/AAAI/article/view/21723},
    doi = {10.1609/aaai.v36i11.21723},
    abstract = {While Math Word Problem (MWP) solving has emerged as a popular field of study and made great progress in recent years, most existing methods are benchmarked solely on one or two datasets and implemented with different configurations. In this paper, we introduce the first open-source library for solving MWPs called MWPToolkit, which provides a unified, comprehensive, and extensible framework for the research purpose. Specifically, we deploy 17 deep learning-based MWP solvers and 6 MWP datasets in our toolkit. These MWP solvers are advanced models for MWP solving, covering the categories of Seq2seq, Seq2Tree, Graph2Tree, and Pre-trained Language Models. And these MWP datasets are popular datasets that are commonly used as benchmarks in existing work. Our toolkit is featured with highly modularized and reusable components, which can help researchers quickly get started and develop their own models. We have released the code and documentation of MWPToolkit in https://github.com/LYH-YF/MWPToolkit.},
    language = {en},
    number = {11},
    urldate = {2023-07-23},
    journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
    author = {Lan, Yihuai and Wang, Lei and Zhang, Qiyuan and Lan, Yunshi and Dai, Bing Tian and Wang, Yan and Zhang, Dongxiang and Lim, Ee-Peng},
    month = jun,
    year = {2022},
    note = {Number: 11},
    keywords = {Toolkit},
    pages = {13188--13190},
}

@article{kaliszyk_mizar_2015,
    title = {{MizAR} 40 for {Mizar} 40},
    volume = {55},
    issn = {0168-7433, 1573-0670},
    url = {http://link.springer.com/10.1007/s10817-015-9330-8},
    doi = {10.1007/s10817-015-9330-8},
    language = {en},
    number = {3},
    urldate = {2023-07-10},
    journal = {Journal of Automated Reasoning},
    author = {Kaliszyk, Cezary and Urban, Josef},
    month = oct,
    year = {2015},
    pages = {245--256},
}

@inproceedings{jiang_lisa_2021,
    title = {{LISA}: {Language} models of {ISAbelle} proofs},
    abstract = {We introduce an environment that allows interaction with an Isabelle server in an incremental manner. With this environment, we mined the Isabelle standard library and the Archive of Formal Proofs (AFP) and extracted 183K lemmas and theorems. We built language models on this large corpus and showed their effectiveness in proving AFP theorems.},
    language = {en},
    booktitle = {6th {Conference} on {Artificial} {Intelligence} and {Theorem} {Proving}},
    author = {Jiang, Albert Qiaochu and Li, Wenda and Han, Jesse Michael and Wu, Yuhuai},
    year = {2021},
}


@inproceedings{
kaliszyk_holstep_2017,
    title = {HolStep: A Machine Learning Dataset for Higher-order Logic Theorem Proving},
    author = {Cezary Kaliszyk and Fran{\c{c}}ois Chollet and Christian Szegedy},
    booktitle = {International Conference on Learning Representations},
    year = {2017},
    url = {https://openreview.net/forum?id=ryuxYmvel}
}


@inproceedings{
polu_formal_2022,
    title = {Formal Mathematics Statement Curriculum Learning},
    author = {Stanislas Polu and Jesse Michael Han and Kunhao Zheng and Mantas Baksys and Igor Babuschkin and Ilya Sutskever},
    booktitle = {The Eleventh International Conference on Learning Representations },
    year = {2023},
    url = {https://openreview.net/forum?id=-P7G-8dmSh4}
}


@article{gauthier_tactictoe_2021,
    title = {{TacticToe}: {Learning} to {Prove} with {Tactics}},
    volume = {65},
    issn = {1573-0670},
    shorttitle = {{TacticToe}},
    url = {https://doi.org/10.1007/s10817-020-09580-x},
    doi = {10.1007/s10817-020-09580-x},
    abstract = {We implement an automated tactical prover TacticToe on top of the HOL4 interactive theorem prover. TacticToe learns from human proofs which mathematical technique is suitable in each proof situation. This knowledge is then used in a Monte Carlo tree search algorithm to explore promising tactic-level proof paths. On a single CPU, with a time limit of 60 s, TacticToe proves 66.4\% of the 7164 theorems in HOL4’s standard library, whereas E prover with auto-schedule solves 34.5\%. The success rate rises to 69.0\% by combining the results of TacticToe and E prover.},
    language = {en},
    number = {2},
    urldate = {2023-06-26},
    journal = {Journal of Automated Reasoning},
    author = {Gauthier, Thibault and Kaliszyk, Cezary and Urban, Josef and Kumar, Ramana and Norrish, Michael},
    month = feb,
    year = {2021},
    keywords = {Formalization, HOL4, Interactive theorem prover, Machine learning, Policy, Tactic, Theorem, Tree search},
    pages = {257--286},
}


@article{bansal_learning_2019,
    title = {Learning to {Reason} in {Large} {Theories} without {Imitation}},
    copyright = {arXiv.org perpetual, non-exclusive license},
    url = {https://arxiv.org/abs/1905.10501},
    doi = {10.48550/ARXIV.1905.10501},
    abstract = {In this paper, we demonstrate how to do automated theorem proving in the presence of a large knowledge base of potential premises without learning from human proofs. We suggest an exploration mechanism that mixes in additional premises selected by a tf-idf (term frequency-inverse document frequency) based lookup in a deep reinforcement learning scenario. This helps with exploring and learning which premises are relevant for proving a new theorem. Our experiments show that the theorem prover trained with this exploration mechanism outperforms provers that are trained only on human proofs. It approaches the performance of a prover trained by a combination of imitation and reinforcement learning. We perform multiple experiments to understand the importance of the underlying assumptions that make our exploration approach work, thus explaining our design choices.},
    urldate = {2023-06-26},
    author = {Bansal, Kshitij and Szegedy, Christian and Rabe, Markus N. and Loos, Sarah M. and Toman, Viktor},
    year = {2019},
    note = {arXiv:1905.10501},
    keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, Logic in Computer Science (cs.LO), Machine Learning (cs.LG), Machine Learning (stat.ML)},
}

@article{paliwal_graph_2020,
    title = {Graph {Representations} for {Higher}-{Order} {Logic} and {Theorem} {Proving}},
    volume = {34},
    copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
    issn = {2374-3468},
    url = {https://ojs.aaai.org/index.php/AAAI/article/view/5689},
    doi = {10.1609/aaai.v34i03.5689},
    abstract = {This paper presents the first use of graph neural networks (GNNs) for higher-order proof search and demonstrates that GNNs can improve upon state-of-the-art results in this domain. Interactive, higher-order theorem provers allow for the formalization of most mathematical theories and have been shown to pose a significant challenge for deep learning. Higher-order logic is highly expressive and, even though it is well-structured with a clearly defined grammar and semantics, there still remains no well-established method to convert formulas into graph-based representations. In this paper, we consider several graphical representations of higher-order logic and evaluate them against the HOList benchmark for higher-order theorem proving.},
    language = {en},
    number = {03},
    urldate = {2023-06-26},
    journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
    author = {Paliwal, Aditya and Loos, Sarah and Rabe, Markus and Bansal, Kshitij and Szegedy, Christian},
    month = apr,
    year = {2020},
    note = {Number: 03},
    pages = {2967--2974},
}

@article{leroy_compcert_2014,
    title = {The {CompCert} {C} verified compiler: {Documentation} and user’s manual},
    shorttitle = {The {CompCert} {C} verified compiler},
    abstract = {This document is the user’s manual for the CompCert C verified compiler. It is organized as follows: Chapter 1 gives an overview of the CompCert C compiler and of the formal verification of compilers. Chapter 2 explains how to install CompCert C. Chapter 3 explains how to use the CompCert C compiler. Chapter 4 explains how to use the CompCert C reference interpreter. Chapter 5 describes the subset of the ISO C99 language that is implemented by CompCert. Chapter 6 describes the supported language extensions: pragmas, attributes, built-in functions. Chapter 7 describes the experimental tool cchecklink that validates a posteriori the correctness of assembling and linking the code produced by the CompCert C compiler.},
    author = {Leroy, Xavier},
    month = sep,
    year = {2014},
    note = {Avalailable at https://compcert.org/man/manual.pdf}
}

@inproceedings{gonthier_four_2008,
    address = {Berlin, Heidelberg},
    series = {Lecture {Notes} in {Computer} {Science}},
    title = {The {Four} {Colour} {Theorem}: {Engineering} of a {Formal} {Proof}},
    isbn = {978-3-540-87827-8},
    shorttitle = {The {Four} {Colour} {Theorem}},
    doi = {10.1007/978-3-540-87827-8_28},
    abstract = {The 150 year old Four Colour Theorem is the first famous result with a proof that requires large computer calculations. Such proofs are still controversial: It is thought that computer programs cannot be reviewed with mathematical rigor.},
    language = {en},
    booktitle = {Computer {Mathematics}},
    publisher = {Springer},
    author = {Gonthier, Georges},
    editor = {Kapur, Deepak},
    year = {2008},
    pages = {333--333},
}


@inproceedings{klein_sel4_2009,
    address = {New York, NY, USA},
    series = {{SOSP} '09},
    title = {{seL4}: formal verification of an {OS} kernel},
    isbn = {978-1-60558-752-3},
    shorttitle = {{seL4}},
    url = {https://doi.org/10.1145/1629575.1629596},
    doi = {10.1145/1629575.1629596},
    abstract = {Complete formal verification is the only known way to guarantee that a system is free of programming errors. We present our experience in performing the formal, machine-checked verification of the seL4 microkernel from an abstract specification down to its C implementation. We assume correctness of compiler, assembly code, and hardware, and we used a unique design approach that fuses formal and operating systems techniques. To our knowledge, this is the first formal proof of functional correctness of a complete, general-purpose operating-system kernel. Functional correctness means here that the implementation always strictly follows our high-level abstract specification of kernel behaviour. This encompasses traditional design and implementation safety properties such as the kernel will never crash, and it will never perform an unsafe operation. It also proves much more: we can predict precisely how the kernel will behave in every possible situation. seL4, a third-generation microkernel of L4 provenance, comprises 8,700 lines of C code and 600 lines of assembler. Its performance is comparable to other high-performance L4 kernels.},
    urldate = {2023-06-25},
    booktitle = {Proceedings of the {ACM} {SIGOPS} 22nd symposium on {Operating} systems principles},
    publisher = {Association for Computing Machinery},
    author = {Klein, Gerwin and Elphinstone, Kevin and Heiser, Gernot and Andronick, June and Cock, David and Derrin, Philip and Elkaduwe, Dhammika and Engelhardt, Kai and Kolanski, Rafal and Norrish, Michael and Sewell, Thomas and Tuch, Harvey and Winwood, Simon},
    month = oct,
    year = {2009},
    keywords = {isabelle/hol, l4, microkernel, sel4},
    pages = {207--220},
}

@article{tan_verified_2019,
    title = {The verified {CakeML} compiler backend},
    volume = {29},
    issn = {0956-7968, 1469-7653},
    url = {https://www.cambridge.org/core/journals/journal-of-functional-programming/article/verified-cakeml-compiler-backend/E43ED3EA740D2DF970067F4E2BB9EF7D},
    doi = {10.1017/S0956796818000229},
    abstract = {The CakeML compiler is, to the best of our knowledge, the most realistic verified compiler for a functional programming language to date. The architecture of the compiler, a sequence of intermediate languages through which high-level features are compiled away incrementally, enables verification of each compilation pass at an appropriate level of semantic detail. Parts of the compiler’s implementation resemble mainstream (unverified) compilers for strict functional languages, and it supports several important features and optimisations. These include efficient curried multi-argument functions, configurable data representations, efficient exceptions, register allocation, and more. The compiler produces machine code for five architectures: x86-64, ARMv6, ARMv8, MIPS-64, and RISC-V. The generated machine code contains the verified runtime system which includes a verified generational copying garbage collector and a verified arbitrary precision arithmetic (bignum) library. In this paper, we present the overall design of the compiler backend, including its 12 intermediate languages. We explain how the semantics and proofs fit together and provide detail on how the compiler has been bootstrapped inside the logic of a theorem prover. The entire development has been carried out within the HOL4 theorem prover.},
    language = {en},
    urldate = {2023-06-26},
    journal = {Journal of Functional Programming},
    author = {Tan, Yong Kiam and Myreen, Magnus O. and Kumar, Ramana and Fox, Anthony and Owens, Scott and Norrish, Michael},
    month = jan,
    year = {2019},
    note = {Publisher: Cambridge University Press},
    pages = {e2},
}


@inproceedings{wang_premise_2017,
    title = {Premise {Selection} for {Theorem} {Proving} by {Deep} {Graph} {Embedding}},
    volume = {30},
    url = {https://papers.nips.cc/paper_files/paper/2017/hash/18d10dc6e666eab6de9215ae5b3d54df-Abstract.html},
    abstract = {We propose a deep learning-based approach to the problem of premise selection: selecting mathematical statements relevant for proving a given conjecture. We represent a higher-order logic formula as a graph that is invariant to variable renaming but still fully preserves syntactic and semantic information. We then embed the graph into a vector via a novel embedding method that preserves the information of edge ordering. Our approach achieves state-of-the-art results on the HolStep dataset, improving the classification accuracy from 83\% to 90.3\%.},
    urldate = {2023-06-26},
    booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
    publisher = {Curran Associates, Inc.},
    author = {Wang, Mingzhe and Tang, Yihe and Wang, Jian and Deng, Jia},
    year = {2017},
}


@inproceedings{
lample_hypertree_2022,
    title = {HyperTree Proof Search for Neural Theorem Proving},
    author = {Guillaume Lample and Timothee Lacroix and Marie-anne Lachaux and Aurelien Rodriguez and Amaury Hayat and Thibaut Lavril and Gabriel Ebner and Xavier Martinet},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
    year = {2022},
    url = {https://openreview.net/forum?id=J4pX8Q8cxHH}
}

@misc{crouse_improving_2020,
    title = {Improving {Graph} {Neural} {Network} {Representations} of {Logical} {Formulae} with {Subgraph} {Pooling}},
    url = {http://arxiv.org/abs/1911.06904},
    abstract = {Recent advances in the integration of deep learning with automated theorem proving have centered around the representation of logical formulae as inputs to deep learning systems. In particular, there has been a growing interest in adapting structure-aware neural methods to work with the underlying graph representations of logical expressions. While more effective than character and token-level approaches, graph-based methods have often made representational trade-offs that limited their ability to capture key structural properties of their inputs. In this work we propose a novel approach for embedding logical formulae that is designed to overcome the representational limitations of prior approaches. Our architecture works for logics of different expressivity; e.g., ﬁrst-order and higher-order logic. We evaluate our approach on two standard datasets and show that the proposed architecture achieves state-of-the-art performance on both premise selection and proof step classiﬁcation.},
    language = {en},
    urldate = {2023-06-26},
    publisher = {arXiv},
    author = {Crouse, Maxwell and Abdelaziz, Ibrahim and Cornelio, Cristina and Thost, Veronika and Wu, Lingfei and Forbus, Kenneth and Fokoue, Achille},
    month = jun,
    year = {2020},
    note = {arXiv:1911.06904},
    keywords = {Computer Science - Artificial Intelligence, Computer Science - Logic in Computer Science, Computer Science - Machine Learning, Computer Science - Symbolic Computation},
}


@inproceedings{
luo_transformers_2023,
    title = {Transformers over Directed Acyclic Graphs},
    author = {Yuankai Luo and Veronika Thost and Lei Shi},
    booktitle = {Thirty-seventh Conference on Neural Information Processing Systems},
    year = {2023},
    url = {https://openreview.net/forum?id=g49s1N5nmO}
}


@inproceedings{chen_structure-aware_2022,
    title = {Structure-Aware Transformer for Graph Representation Learning},
    author = {Dexiong Chen and Leslie O’Bray and Karsten M. Borgwardt},
    booktitle = {International Conference on Machine Learning},
    year = {2022},
    url = {https://api.semanticscholar.org/CorpusID:246634635}
}



