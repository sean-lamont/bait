# @package _global_
defaults:
  - hol4
  - /data_type/sequence@_here_

exp_config:
  name: transformer_encoder

model:
  goal_embedding_module:
    _target_: models.embedding_models.transformer.transformer_encoder_model.TransformerWrapper
    # input_shape passed in from data config
    ntoken: ${data_config.vocab_size}
    d_model: 256
    nhead: 4
    nlayers: 2
    dropout: 0.
    d_hid: 256
    small_inner: false
    max_len: ${data_config.attributes.max_len}

  premise_embedding_module:
    _target_: models.embedding_models.transformer.transformer_encoder_model.TransformerWrapper
    # input_shape passed in from data config
    ntoken: ${data_config.vocab_size}
    d_model: 256
    nhead: 4
    nlayers: 2
    dropout: 0.
    d_hid: 256
    small_inner: false
    max_len: ${data_config.attributes.max_len}