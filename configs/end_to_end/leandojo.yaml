# @package _global_
defaults:
  - /base/logging_config@_here_
  - /base/exp_config@_here_

exp_config:
  name: test
  experiment: test

logging_config:
  project: end_to_end
  offline: true

num_theorems: 10 #94000

# environment to use (leandojo, holist)
env_config:
  env: leandojo
  data_path: data/LeanDojo/data/leandojo_benchmark/random/
  #split: train
  split: test
  file_path: null
  full_name: null
  name_filter: null
  num_theorems: ${num_theorems} #10 #94000

  name: null
  indexed_corpus_path: null

# whether to shuffle the loaded theorems before evaluation
shuffle: false

# Number of End-to-End Eval -> Train -> Eval loops
num_iterations: 2
resume_iteration: 0

# Total time allowed for a single proof attempt
total_timeout: 6000

# Maximum time allowed in environment before timing out
env_timeout: 10

with_gpus: true
log_level: 'INFO'

# Resource configuration
# number of physical GPUs
num_gpus: 1
# how many 'logical' GPUs available, should be a multiple of num_gpus.
# Will control the number of separate tactic/search model processes
logical_gpus: 1
num_cpus: 16
# How much GPU memory to allocate per prover (should be low, since the model is using most of the GPU)
gpu_per_prover: 0.01
# CPU resources to allocate per prover.
cpu_per_prover: 1
# How many provers to assign per logical GPU. Each of these provers will share one tactic/search process.
provers_per_gpu: 1

num_sampled_tactics: 64

tac_model:
  model: reprover
  #  ckpt_path: runs/gen.ckpt
  distributed: true
  gpu_per_process: 0.45
  cpu_per_process: 1
  config:
    model_name: kaiyuy/leandojo-lean3-tacgen-byt5-small
    lr: 5e-6
    warmup_steps: 200
    length_penalty: 0.0
    num_beams: 64
    ret_ckpt_path: null
    max_seq_len: 2300
    gen_config:
      strategy: beam
      length_penalty: 0.0
    # configuration for the eval loop in training (terminates based on live proving performance)
    eval_config: null
    #      eval_num_theorems: 200
    #      shuffle: false
    #      timeout: 600
    # configuration for LoRA models
    #lora_config:
    #  target_modules: ['q', 'k', 'v', 'o', 'wo', 'lm_head']
    #  task_type: "SEQ_2_SEQ_LM"
    #  r: 16
    #  lora_alpha: 16
    #  lora_dropout: 0.01

search_model:
  search: bestfs

#  search: updown
#  ckpt_path: runs/goal_model.ckpt
#  distributed: true
#  gpu_per_process: 0.45 #0.225
#  cpu_per_process: 1 # 0.5

#  search: htps
#  ckpt_path: goal_model.ckpt # goal_step.ckpt
#  distributed: true
#  gpu_per_process: 0.45
#  cpu_per_process: 0.5
#  exploration_constant: 1

# Commands to run for retraining the model(s) after every evaluation
train_after_eval:
#  - python3 -m experiments.lightning_runner --config-name=end_to_end/train/goal_model/run data_module.trace_files=${exp_config.directory}/traces
  - python3 -m experiments.lightning_runner --config-name=end_to_end/train/gen_seq2seq/run data_module.trace_files=${exp_config.directory}/traces
# The model and its checkpoint attribute to update.
# Must be same length as train_after_eval, with each index corresponding to the associated command
update_checkpoints:
#  - [ search_model, ckpt_path ]
  - [tac_model, ckpt_path]