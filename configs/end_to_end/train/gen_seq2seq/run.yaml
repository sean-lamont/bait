# @package _global_
defaults:
  - /base/logging_config@_here_
  - /base/exp_config@_here_

exp_config:
  name: seq2seq_detailed_2
  experiment: end_to_end

logging_config:
  project: end_to_end
  offline: false

model:
  _target_: models.end_to_end.tactic_models.generator.model.RetrievalAugmentedGenerator
  config:
    model_name: kaiyuy/leandojo-lean3-tacgen-byt5-small
    #    lr: 1e-6
    # learning rate used in HTPS is 3e-5
    lr: 1e-5
    warmup_steps: 150
    beta: 0.99
    max_seq_len: 2300


    num_val_samples: 64

    # retriever path
    ret_ckpt_path: null

    # should always be beam search
    gen_config:
      strategy: beam
      length_penalty: 0.0


    # config for live evaluation
    live_eval: true
    eval_config:
      eval_num_theorems: 300
      shuffle: false
      timeout: 600
      # frequency of live evaluation in epochs
      frequency: 1

    lora_config:
      target_modules: [ 'q', 'k', 'v', 'o', 'wo', 'lm_head' ]
      task_type: "SEQ_2_SEQ_LM"
      r: 16
      lora_alpha: 16
      lora_dropout: 0.01

data_module:
  _target_: models.end_to_end.tactic_models.generator.datamodule.GeneratorDataModule
  model_name: kaiyuy/leandojo-lean3-tacgen-byt5-small
  batch_size: 2  # effective_batch_size == batch_size * accumulate_grad_batches * devices
  eval_batch_size: 2
  max_seq_len: 2300
  num_workers: 0

  trace_files: runs/end_to_end/original/2024_02_08/09_55_29/traces/0
  # old 60% traces
  #runs/original_reprover/original_reprover_2023_12_10/20_32_14/traces
  collection: seq2seq_0

#  trace_files: null
#  collection: null
#  database: null
trainer:
  accelerator: gpu
  devices: 1
  precision: bf16-mixed
  strategy:
    _target_: lightning.pytorch.strategies.DeepSpeedStrategy
    stage: 2
    offload_optimizer: false
    cpu_checkpointing: false
    logging_batch_size_per_gpu: 2
  gradient_clip_val: 1.0
  max_steps: 5000000
  #  val_check_interval: 10
  callbacks:
    - _target_: lightning.pytorch.callbacks.LearningRateMonitor
      logging_interval: step
    - _target_: lightning.pytorch.callbacks.ModelCheckpoint
      verbose: true
      save_top_k: 3
      save_last: true
      monitor: Pass@1_val
#      monitor: top64_acc_val
      mode: max
      dirpath: ${exp_config.directory}/checkpoints
      auto_insert_metric_name: true
      filename: "{epoch}-{step}-{Pass@1_val:.2f}"
#      filename: "{epoch}-{step}-{top64_acc:.2f}"
# todo add early stopping
