defaults:
  - /base/data_config@_here_
  - /base/optimiser_config@_here_
  - /base/logging_config@_here_
  - /base/exp_config@_here_

exp_config:
  experiment: holist_eval

data_config:
  batch_size: 16
  data_options:
    db: 'holist'

model_config:
  model_type: ???
  model_attributes:
    vocab_size: 1500

logging_config:
  project: holist_eval
  offline: true

# Setting ProverOptions Proto
path_tactics: 'experiments/HOList/config/hollight_tactics.textpb'
path_tactics_replace: 'experiments/HOList/config/hollight_tactics_replacements.textpb'
path_theorem_database: 'data/HOList/theorem_database_v1.1.textpb'

model_architecture: PARAMETERS_CONDITIONED_ON_TAC
theorem_embeddings: runs/holist_eval/bow_asm_only_2024_01_22/12_12_46/embeddings/checkpoint.npy # ${exp_config.directory}/embeddings/checkpoint.npy
path_model_prefix: runs/holist_supervised/gnn_small_2024_01_22/12_00_20/checkpoints/last #${exp_config.directory}/checkpoints/checkpoint

tactic_timeout_ms: 5000

max_theorem_parameters: 20

action_generator_options:
#  asm_meson_no_params_only: true
# asm_meson_only: true
  max_theorem_parameters: 20

prover: 'bfs'

bfs_options:
  max_top_suggestions: 5 # k1
  max_successful_branches: 5
  max_explored_nodes: 20

# Standard validation setup

# Specifies which examples to run on. Either "all" or comma separated list
# of library tags. This setting overrides the related setting in the
#library_tags: "complex"
# ProverOptions protobuf.
libraries: "complex"

# Specifies which examples to run on. Either "all" or comma separated list
# of {"training, "testing" and "validation"} This setting overrides the
# related setting in the ProverOptions protobuf.
splits: 'validation'


# Old tf Flags overrides

# Optional ProverTaskList text protobuf to specify the theorem proving
# tasks. If not supplied then a task list is generated automatically from
# the theorem library. The filtering for training_split is in effect for
# the goals in task_list as well.
task_list: False

# Optional multi-line ProverTask text protobuf or recordio to specify the
# theorem proving tasks. Either this or task list or tasks_by_fingerprint
# must be specified, otherwise the tasks are generated automatically from
# the theorem library. The filtering for training_split is in effect for
# the goals in the read tasks as well.
tasks: False

#'Optional comma-separated list of fingerprints of theorems in the theorem '
#'database. No filtering by training_split in place.
tasks_by_fingerprint: False

# Override the timeout/task specified in the prover options.
timeout_seconds: 20

# Path where proof logs are saved.
output: ${exp_config.checkpoint_dir}/proof_logs.textpbs

# frequency to save logs and eval progress
log_frequency: 10

done_output: ${exp_config.checkpoint_dir}/tasks_done.textpbs